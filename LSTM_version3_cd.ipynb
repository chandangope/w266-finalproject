{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "143613/143613 [==============================] - 1069s 7ms/step - loss: 0.2844 - acc: 0.9907 - val_loss: 0.2791 - val_acc: 0.9939\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.27913, saving model to weights_base.best.hdf5\n",
      "Epoch 2/2\n",
      "143613/143613 [==============================] - 1116s 8ms/step - loss: 0.2668 - acc: 0.9926 - val_loss: 0.2764 - val_acc: 0.9896\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.27913 to 0.27637, saving model to weights_base.best.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x128f59c50>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Embedding, Input\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "max_features = 20000\n",
    "maxlen = 100\n",
    "\n",
    "\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "#test = pd.read_csv(\"test.csv\")\n",
    "#train = train.sample(frac=1)\n",
    "\n",
    "list_sentences_train = train[\"comment_text\"].fillna(\"nada\").values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "#list_sentences_test = test[\"comment_text\"].fillna(\"nada\").values\n",
    "\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "#list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_t = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "#X_te = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "\n",
    "def get_model():\n",
    "    embed_size = 128\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size)(inp)\n",
    "    x = Bidirectional(LSTM(50, return_sequences=True))(x)\n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(50, activation=\"relu\")(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(6, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model = get_model()\n",
    "batch_size = 32\n",
    "epochs = 2\n",
    "\n",
    "\n",
    "file_path=\"weights_base.best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=20)\n",
    "\n",
    "\n",
    "callbacks_list = [checkpoint, early] #early\n",
    "model.fit(X_t, y, batch_size=batch_size, epochs=epochs, validation_split=0.1, callbacks=callbacks_list)\n",
    "\n",
    "#model.load_weights(file_path)\n",
    "\n",
    "#y_predict = model.predict(X_te)\n",
    "\n",
    "\n",
    "\n",
    "#sample_submission = pd.read_csv(\"sample_submission.csv\")\n",
    "\n",
    "#sample_submission[list_classes] = y_predict\n",
    "\n",
    "\n",
    "\n",
    "#sample_submission.to_csv(\"baseline.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#maxlens=[100,200,300]\n",
    "#batch_sizes=[32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_len_bin=dict()\n",
    "\n",
    "comment_len_bin['total']=train.shape[0]\n",
    "comment_len_bin['100']=0\n",
    "comment_len_bin['150']=0\n",
    "comment_len_bin['200']=0\n",
    "comment_len_bin['250']=0\n",
    "comment_len_bin['300']=0\n",
    "\n",
    "for index,row in train.iterrows():\n",
    "    \n",
    "    if len(row['comment_text'].split(' '))<=100:\n",
    "        comment_len_bin['100']+=(1)\n",
    "        \n",
    "    if len(row['comment_text'].split(' '))<=150:\n",
    "        comment_len_bin['150']+=(1)\n",
    "\n",
    "    if len(row['comment_text'].split(' '))<=200:\n",
    "        comment_len_bin['200']+=(1)\n",
    "\n",
    "    if len(row['comment_text'].split(' '))<=250:\n",
    "        comment_len_bin['250']+=(1)\n",
    "\n",
    "    if len(row['comment_text'].split(' '))<=300:\n",
    "        comment_len_bin['300']+=(1)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'100': 130994, '150': 0, '200': 0, '250': 0, '300': 0, 'total': 159571}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment_len_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 82.09 % of comments have length equal or less than 100 words\n",
      " 89.78 % of comments have length equal or less than 150 words\n",
      " 93.61 % of comments have length equal or less than 200 words\n",
      " 95.75 % of comments have length equal or less than 250 words\n",
      " 96.95 % of comments have length equal or less than 300 words\n"
     ]
    }
   ],
   "source": [
    "for i in ['100','150','200','250','300']:\n",
    "    print('{:6.2f}'.format(100*comment_len_bin[i]/comment_len_bin['total']),'% of comments have length equal or less than', i, 'words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets Automate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget('https://s3.amazonaws.com/danicic-w266-final/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ChrisD/anaconda3/envs/tensorflow/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from scipy.sparse import hstack\n",
    "from sklearn import model_selection\n",
    "import numpy as np\n",
    "import tensorflow  as tf\n",
    "import keras\n",
    "import h5py\n",
    "from keras.models import load_model\n",
    "\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Embedding, Input\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, MaxPool1D, Dropout\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from keras.metrics import categorical_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing data...\n",
      "Splitting data...\n",
      "127656 train sequences\n",
      "31915 test sequences\n"
     ]
    }
   ],
   "source": [
    "# import data\n",
    "np.random.seed(412)\n",
    "\n",
    "print('Importing data...')\n",
    "train_all=pd.read_csv('train.csv')\n",
    "\n",
    "# 80/20 split\n",
    "train_80, test_20 = model_selection.train_test_split(train_all,test_size=0.2)\n",
    "\n",
    "#preprocessing\n",
    "\n",
    "print('Splitting data...')\n",
    "\n",
    "#training data\n",
    "x_train = train_80['comment_text'].fillna(\"nada\").values\n",
    "y_train = train_80[['toxic','severe_toxic','obscene','threat','insult','identity_hate']].values\n",
    "\n",
    "#testing validation data (not for training model, just validation for )\n",
    "\n",
    "x_test = test_20['comment_text'].fillna(\"nada\").values\n",
    "y_test = test_20[['toxic','severe_toxic','obscene','threat','insult','identity_hate']].values\n",
    "\n",
    "# check lengths\n",
    "\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_feature_list=[20000,10000]\n",
    "EPOCHS=4\n",
    "maxlen_list=[150,250] \n",
    "dropout_list=[0.1,0.2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer = text.Tokenizer(num_words=max_feature_list[0])\n",
    "\n",
    "#tokenizer.fit_on_texts(list(train_all[\"comment_text\"].fillna(\"nada\").values)) #fit on all comment_text\n",
    "\n",
    "##create tokenized comments\n",
    "#list_tokenized_train = tokenizer.texts_to_sequences(x_train)  #training (80% of train.csv)\n",
    "#list_tokenized_test = tokenizer.texts_to_sequences(x_test)  #testing (20% of train.csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*using 20000 max features*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create LSTM model maker function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(maxlen,max_features,dropout,embed_size=128):\n",
    "    \n",
    "    embed_size=embed_size #default to 128\n",
    "    maxlen=maxlen # max length of sequence input\n",
    "    max_features=max_features  # max vocab\n",
    "    dropout=dropout\n",
    "    \n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    \n",
    "    x = Embedding(max_features, embed_size)(inp)\n",
    "    \n",
    "    x = Bidirectional(LSTM(50, return_sequences=False))(x)\n",
    "    \n",
    "    #x = GlobalMaxPool1D()(x)\n",
    "    \n",
    "    #x = Dropout(dropout)(x)\n",
    "    \n",
    "    x = Dense(50, activation=\"relu\")(x)\n",
    "    \n",
    "    x = Dropout(dropout)(x)\n",
    "    \n",
    "    x = Dense(6, activation=\"sigmoid\")(x)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=[categorical_accuracy])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize and fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizing\n",
      "creating sequences\n",
      "padding maxlen= 250\n",
      "building model\n",
      "fitting model with max_features=10000, maxlen = 250, and dropout = 0.1\n",
      "Train on 114890 samples, validate on 12766 samples\n",
      "Epoch 1/4\n",
      "114890/114890 [==============================] - 1787s 16ms/step - loss: 0.0683 - categorical_accuracy: 0.9487 - val_loss: 0.0522 - val_categorical_accuracy: 0.9930\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.05222, saving model to weights_base_10000_250_0.1.hdf5\n",
      "Epoch 2/4\n",
      "114890/114890 [==============================] - 1664s 14ms/step - loss: 0.0477 - categorical_accuracy: 0.9809 - val_loss: 0.0499 - val_categorical_accuracy: 0.9930\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.05222 to 0.04993, saving model to weights_base_10000_250_0.1.hdf5\n",
      "Epoch 3/4\n",
      "114890/114890 [==============================] - 1671s 15ms/step - loss: 0.0422 - categorical_accuracy: 0.9520 - val_loss: 0.0499 - val_categorical_accuracy: 0.9930\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/4\n",
      "114890/114890 [==============================] - 1712s 15ms/step - loss: 0.0370 - categorical_accuracy: 0.9272 - val_loss: 0.0542 - val_categorical_accuracy: 0.9853\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n"
     ]
    }
   ],
   "source": [
    "max_feature_list=[20000,10000]\n",
    "maxlen_list=[150,250] \n",
    "dropout_list=[0.1,0.2]\n",
    "EPOCHS=4\n",
    "\n",
    "for max_feature in [max_feature_list[1]]:\n",
    "    \n",
    "    print('tokenizing')\n",
    "    tokenizer = text.Tokenizer(num_words=max_feature)\n",
    "\n",
    "    tokenizer.fit_on_texts(list(train_all[\"comment_text\"].fillna(\"nada\").values)) #fit on all comment_text\n",
    "\n",
    "    #create tokenized comments\n",
    "    print('creating sequences')\n",
    "    list_tokenized_train = tokenizer.texts_to_sequences(x_train)  #training (80% of train.csv)\n",
    "    list_tokenized_test = tokenizer.texts_to_sequences(x_test)  #testing (20% of train.csv)\n",
    "    \n",
    "    for maxlen in [maxlen_list[1]]:\n",
    "        \n",
    "        for dropout in [dropout_list[0]]:\n",
    "            \n",
    "            print('padding maxlen=',maxlen)\n",
    "            x_train_pad = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "            x_test_pad = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "            \n",
    "            print('building model')\n",
    "            model=get_model(maxlen=maxlen,max_features=max_feature,dropout=dropout,embed_size=128)\n",
    "            \n",
    "            batch_size = 32\n",
    "\n",
    "\n",
    "            weight_file_path=\"weights_base_{}_{}_{}.hdf5\".format(max_feature,maxlen,dropout)\n",
    "            model_file_path='bidirectional_lstm_globMP_relu_sigmoid_maxfeat{}_maxlen{}_dropout{}'.format(max_feature,maxlen,dropout)\n",
    "\n",
    "            checkpoint = ModelCheckpoint(weight_file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "            early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=20)\n",
    "\n",
    "            callbacks_list = [checkpoint, early] #early\n",
    "            \n",
    "            #fit model\n",
    "            print('fitting model with max_features={}, maxlen = {}, and dropout = {}'.format(max_feature,maxlen,dropout))\n",
    "            model.fit(x_train_pad, y_train, batch_size=batch_size, epochs=EPOCHS, validation_split=0.1, callbacks=callbacks_list)\n",
    "            \n",
    "            #save model\n",
    "            model.save(model_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[array([[-0.03326945, -0.03104633, -0.0160776 , ..., -0.02599823,\n",
      "         0.0395217 , -0.0412238 ],\n",
      "       [-0.01143156,  0.04759712,  0.00566554, ..., -0.00271648,\n",
      "        -0.03129635, -0.02996721],\n",
      "       [ 0.04391935, -0.0277747 , -0.03758749, ..., -0.03850917,\n",
      "         0.01456382, -0.04619063],\n",
      "       ...,\n",
      "       [-0.01271094, -0.04688478, -0.01596321, ...,  0.03239927,\n",
      "        -0.0429408 ,  0.00881263],\n",
      "       [ 0.02129759,  0.02610108, -0.02143035, ..., -0.00189376,\n",
      "        -0.0146777 , -0.00344218],\n",
      "       [-0.01640928, -0.04305264, -0.0156445 , ..., -0.01497052,\n",
      "         0.03609885, -0.04355785]], dtype=float32)]\n",
      "[array([[ 0.08282873,  0.00014524, -0.11439466, ..., -0.02569608,\n",
      "        -0.13307238, -0.08856292],\n",
      "       [-0.13233139,  0.03258061, -0.05971719, ...,  0.04307289,\n",
      "        -0.07603861, -0.12951237],\n",
      "       [ 0.10117981, -0.11703214, -0.09279917, ..., -0.1214847 ,\n",
      "         0.01854083, -0.04284104],\n",
      "       ...,\n",
      "       [-0.06357735, -0.12858929, -0.09521776, ...,  0.00714077,\n",
      "         0.12063807, -0.071315  ],\n",
      "       [-0.0954579 ,  0.01950252, -0.083175  , ..., -0.0401004 ,\n",
      "         0.0338883 ,  0.1287871 ],\n",
      "       [ 0.12652361,  0.110845  ,  0.09212571, ..., -0.01620737,\n",
      "        -0.02749635,  0.08904803]], dtype=float32), array([[-0.07993121,  0.02325026, -0.0004874 , ...,  0.05559476,\n",
      "        -0.01663542, -0.00208273],\n",
      "       [-0.04552794, -0.07374048,  0.10189358, ...,  0.12168485,\n",
      "        -0.01443643, -0.11799968],\n",
      "       [-0.15226759, -0.17785233, -0.01654246, ..., -0.01671191,\n",
      "         0.03277003, -0.04844873],\n",
      "       ...,\n",
      "       [-0.09393842,  0.01270469,  0.01274277, ..., -0.00088297,\n",
      "         0.08757027, -0.0222588 ],\n",
      "       [ 0.0058618 , -0.02424229,  0.02052232, ...,  0.15027082,\n",
      "        -0.10648941, -0.042958  ],\n",
      "       [-0.09343193,  0.14990684,  0.09626181, ...,  0.10874647,\n",
      "        -0.0770985 , -0.04730397]], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32), array([[-0.00079942, -0.09625661, -0.02418237, ...,  0.01174098,\n",
      "        -0.001587  ,  0.1156927 ],\n",
      "       [-0.11068012, -0.02398612, -0.04338332, ..., -0.05373394,\n",
      "        -0.01939165, -0.07969518],\n",
      "       [ 0.0377471 ,  0.0766056 ,  0.0347531 , ..., -0.05023987,\n",
      "        -0.07438049,  0.0184619 ],\n",
      "       ...,\n",
      "       [-0.01988509,  0.02431165,  0.07810612, ..., -0.02161703,\n",
      "        -0.02697583,  0.08926319],\n",
      "       [ 0.00456625,  0.01915945,  0.07094352, ...,  0.03198645,\n",
      "        -0.01364731,  0.09851678],\n",
      "       [ 0.00302838, -0.11155154,  0.0136378 , ..., -0.00770555,\n",
      "         0.02443935,  0.07498388]], dtype=float32), array([[-0.04097189,  0.08010539, -0.07615808, ...,  0.09102146,\n",
      "         0.06379101, -0.01771178],\n",
      "       [ 0.02201777,  0.12544976, -0.00562731, ...,  0.01838762,\n",
      "        -0.14114891, -0.07611865],\n",
      "       [-0.06379   , -0.00263891,  0.02957852, ...,  0.07724194,\n",
      "         0.0764835 ,  0.02151879],\n",
      "       ...,\n",
      "       [ 0.10655695, -0.0348211 ,  0.09059928, ...,  0.0037093 ,\n",
      "        -0.01012116,  0.04942119],\n",
      "       [-0.01778378,  0.05466672,  0.05049475, ..., -0.02863978,\n",
      "         0.07653152, -0.07271904],\n",
      "       [ 0.08432298,  0.01191017,  0.07001542, ...,  0.06668368,\n",
      "         0.00805703, -0.01115177]], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)]\n",
      "[array([[-0.15271588,  0.10534991,  0.07630421, ...,  0.07588865,\n",
      "        -0.0128829 , -0.14327168],\n",
      "       [ 0.01668072,  0.16933508, -0.07382274, ...,  0.12479739,\n",
      "         0.00694032,  0.07154347],\n",
      "       [ 0.05849649, -0.03430042, -0.10570665, ..., -0.07495256,\n",
      "        -0.08720183,  0.04289789],\n",
      "       ...,\n",
      "       [ 0.0582111 , -0.16864339, -0.03664966, ..., -0.17182699,\n",
      "        -0.01692954,  0.0861568 ],\n",
      "       [-0.12690696, -0.1241703 ,  0.14047624, ..., -0.12534037,\n",
      "        -0.06817111, -0.04511623],\n",
      "       [-0.00755091, -0.12337809, -0.06349401, ...,  0.18427397,\n",
      "         0.03333516, -0.00253125]], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "      dtype=float32)]\n",
      "[]\n",
      "[array([[-0.12072951,  0.27851444, -0.13339265,  0.06904185,  0.07095572,\n",
      "         0.2726574 ],\n",
      "       [-0.01771799, -0.2881691 ,  0.10734919,  0.25965035, -0.2683134 ,\n",
      "         0.18808955],\n",
      "       [-0.23709083, -0.13003844,  0.26764685,  0.10719278,  0.0073328 ,\n",
      "        -0.18242934],\n",
      "       [ 0.22703701,  0.21245962, -0.154267  ,  0.09580359, -0.07569912,\n",
      "        -0.00308222],\n",
      "       [-0.19088982, -0.21784605, -0.15194371, -0.18283945,  0.17180806,\n",
      "        -0.20548189],\n",
      "       [-0.13683611,  0.27571166,  0.3119632 ,  0.27805936,  0.01517192,\n",
      "        -0.22779587],\n",
      "       [-0.23972261, -0.2987582 ,  0.1181547 ,  0.26596117,  0.11278924,\n",
      "         0.05403772],\n",
      "       [ 0.11952206,  0.14108217,  0.22113812, -0.01100251, -0.07411996,\n",
      "        -0.00946355],\n",
      "       [ 0.2616403 , -0.04241103, -0.29458606, -0.11241254,  0.3205908 ,\n",
      "        -0.24086262],\n",
      "       [-0.18639593, -0.31067145, -0.10316877,  0.09130281,  0.22931397,\n",
      "         0.04208896],\n",
      "       [ 0.12228033, -0.10650064,  0.15652907,  0.32004827, -0.00778317,\n",
      "        -0.23771921],\n",
      "       [ 0.14028412,  0.11302882,  0.08621338, -0.10763513,  0.15450034,\n",
      "         0.27481818],\n",
      "       [-0.24946475,  0.27149218,  0.08658993,  0.18699533,  0.19075018,\n",
      "        -0.0841684 ],\n",
      "       [-0.26025164,  0.16034168, -0.3159349 , -0.12916681, -0.22315072,\n",
      "        -0.30821714],\n",
      "       [ 0.3030386 , -0.28812507,  0.19452292,  0.2711898 ,  0.1382055 ,\n",
      "        -0.28090551],\n",
      "       [ 0.24092114,  0.26167023,  0.26650608, -0.17748827,  0.25774527,\n",
      "         0.01007438],\n",
      "       [-0.14319785, -0.09985079,  0.25063026,  0.0314143 ,  0.12624425,\n",
      "        -0.07259667],\n",
      "       [ 0.29854023, -0.13495393,  0.16168141, -0.04486784,  0.23315299,\n",
      "        -0.24852794],\n",
      "       [-0.26195836, -0.2754979 ,  0.08068186,  0.02471507, -0.25239566,\n",
      "         0.15860787],\n",
      "       [ 0.23090214,  0.24079466, -0.24157801, -0.17707068, -0.01027876,\n",
      "         0.12198636],\n",
      "       [ 0.21548718,  0.24323446, -0.24606085,  0.06985706,  0.12905887,\n",
      "         0.03518164],\n",
      "       [ 0.3033126 , -0.11112963,  0.06462598, -0.28091636,  0.322397  ,\n",
      "        -0.2852942 ],\n",
      "       [-0.03665951,  0.03915814,  0.24367988, -0.1876125 ,  0.17588705,\n",
      "        -0.0147135 ],\n",
      "       [-0.25079957, -0.04882467,  0.17533565,  0.00979802, -0.06902772,\n",
      "         0.02024096],\n",
      "       [-0.26735544,  0.03933716,  0.29111665,  0.02137873, -0.08253649,\n",
      "         0.17092106],\n",
      "       [ 0.16773504, -0.0374887 ,  0.28218412,  0.23866594, -0.29807317,\n",
      "        -0.24083039],\n",
      "       [-0.04309693, -0.23469225,  0.01021227,  0.1822542 ,  0.07364491,\n",
      "         0.08759105],\n",
      "       [-0.07219803,  0.05117667,  0.15644965, -0.02876872, -0.07418466,\n",
      "         0.14395702],\n",
      "       [-0.2813206 ,  0.17737591,  0.28232193, -0.27817994, -0.23947896,\n",
      "        -0.03944275],\n",
      "       [ 0.12581098, -0.23044237, -0.2050888 ,  0.3193456 ,  0.29233015,\n",
      "        -0.32297215],\n",
      "       [-0.02008933, -0.22561064,  0.32307345,  0.08136353, -0.27783132,\n",
      "        -0.2513727 ],\n",
      "       [ 0.3053819 ,  0.1751653 ,  0.28471678,  0.27171826, -0.220673  ,\n",
      "         0.00690052],\n",
      "       [-0.16787146,  0.08624038,  0.2851084 , -0.15630823, -0.20422232,\n",
      "         0.24053538],\n",
      "       [-0.29687437, -0.2049968 ,  0.26394898,  0.29571003,  0.32486904,\n",
      "        -0.04580268],\n",
      "       [ 0.2911244 , -0.21566162, -0.00321504, -0.03402758, -0.17582303,\n",
      "        -0.25760168],\n",
      "       [ 0.12927467,  0.19767427,  0.06823108,  0.28154397, -0.11654629,\n",
      "        -0.0698348 ],\n",
      "       [ 0.26567525,  0.29986984, -0.04329962,  0.1179913 , -0.32133627,\n",
      "         0.3008052 ],\n",
      "       [-0.23546813,  0.1406498 , -0.2452198 ,  0.25876284,  0.23621231,\n",
      "         0.16640079],\n",
      "       [-0.1389706 ,  0.12353188, -0.09965873, -0.3156905 ,  0.32016146,\n",
      "         0.06341892],\n",
      "       [-0.08406532, -0.2685227 , -0.04199961,  0.30361444,  0.18094939,\n",
      "         0.0566389 ],\n",
      "       [ 0.19768482,  0.03575954, -0.17374067, -0.27715278,  0.03197902,\n",
      "        -0.10833304],\n",
      "       [ 0.23022777, -0.09121291, -0.0507628 ,  0.03615195,  0.13672397,\n",
      "         0.238864  ],\n",
      "       [ 0.2638486 ,  0.29741675, -0.17554693,  0.11353603,  0.03920186,\n",
      "        -0.08406235],\n",
      "       [ 0.2433604 ,  0.07309303, -0.14401656, -0.0602763 , -0.04503164,\n",
      "        -0.0160715 ],\n",
      "       [ 0.23311067, -0.21431705,  0.27105445, -0.19136174, -0.13261129,\n",
      "         0.11499414],\n",
      "       [ 0.1385569 , -0.05443737,  0.2962904 ,  0.04729483,  0.08615971,\n",
      "         0.2548353 ],\n",
      "       [ 0.19754153, -0.3095897 , -0.04428083,  0.11556891, -0.08015688,\n",
      "        -0.28092486],\n",
      "       [-0.09304109, -0.1417316 , -0.18577503, -0.30224508,  0.05046204,\n",
      "        -0.19059412],\n",
      "       [-0.20095584, -0.3118135 , -0.08987607,  0.11583409,  0.0061419 ,\n",
      "         0.19328827],\n",
      "       [-0.30039737,  0.02058458, -0.24368402, -0.2219371 , -0.12074105,\n",
      "         0.26090854]], dtype=float32), array([0., 0., 0., 0., 0., 0.], dtype=float32)]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Model' object has no attribute 'Bidirectional'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-23d76f28c233>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBidirectional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Model' object has no attribute 'Bidirectional'"
     ]
    }
   ],
   "source": [
    "model.Bidirectional.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model=get_model(maxlen=100,max_features=20000,dropout=0.1,embed_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model \n",
    "\n",
    "# Returns a compiled model identical to the previous one\n",
    "#model = load_model(\"model_name.hdf5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = model.predict(x_test_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31915, 6)\n",
      "(31915, 6)\n"
     ]
    }
   ],
   "source": [
    "print(y_test.shape)\n",
    "print(y_predict.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model=model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31915/31915 [==============================] - 109s 3ms/step\n",
      "categorical_accuracy: 99.24%\n"
     ]
    }
   ],
   "source": [
    "#binary_accuracy(y_true, y_pred)\n",
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss='binary_crossentropy',optimizer='adam',metrics=[categorical_accuracy])\n",
    "score = loaded_model.evaluate(x_test_pad, y_test, verbose=1)\n",
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict={'bidirectional_lstm_globMP_relu_sigmoid_maxfeat20000_maxlen150_dropout0.1':[150,0.1],\\\n",
    "'bidirectional_lstm_globMP_relu_sigmoid_maxfeat20000_maxlen150_dropout0.2':[150,0.2],\\\n",
    "'bidirectional_lstm_globMP_relu_sigmoid_maxfeat20000_maxlen250_dropout0.1':[250,0.1],\\\n",
    "'bidirectional_lstm_globMP_relu_sigmoid_maxfeat20000_maxlen250_dropout0.2':[250,0.2]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------- \n",
      "Model with maxlen of 150 and dropout of 0.1\n",
      "padding maxlen= 150\n",
      "loading model\n",
      "compiling model\n",
      "evaluating model\n",
      "31915/31915 [==============================] - 67s 2ms/step\n",
      "categorical_accuracy: 99.11%\n",
      "-------------------------------------------------- \n",
      "\n",
      "-------------------------------------------------- \n",
      "Model with maxlen of 150 and dropout of 0.2\n",
      "padding maxlen= 150\n",
      "loading model\n",
      "compiling model\n",
      "evaluating model\n",
      "31915/31915 [==============================] - 84s 3ms/step\n",
      "categorical_accuracy: 99.26%\n",
      "-------------------------------------------------- \n",
      "\n",
      "-------------------------------------------------- \n",
      "Model with maxlen of 250 and dropout of 0.1\n",
      "padding maxlen= 250\n",
      "loading model\n",
      "compiling model\n",
      "evaluating model\n",
      "31915/31915 [==============================] - 127s 4ms/step\n",
      "categorical_accuracy: 97.69%\n",
      "-------------------------------------------------- \n",
      "\n",
      "-------------------------------------------------- \n",
      "Model with maxlen of 250 and dropout of 0.2\n",
      "padding maxlen= 250\n",
      "loading model\n",
      "compiling model\n",
      "evaluating model\n",
      "31915/31915 [==============================] - 120s 4ms/step\n",
      "categorical_accuracy: 99.24%\n",
      "-------------------------------------------------- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model \n",
    "\n",
    "# Returns a compiled model identical to the previous one\n",
    "#model = load_model(\"model_name.hdf5\")\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=20000)\n",
    "#tokenize based on comment database\n",
    "tokenizer.fit_on_texts(list(train_all[\"comment_text\"].fillna(\"nada\").values)) #fit on all comment_text\n",
    "#create tokenized comments\n",
    "#list_tokenized_train = tokenizer.texts_to_sequences(x_train)  #training (80% of train.csv)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(x_test)  #testing (20% of train.csv)\n",
    "\n",
    "for file in model_dict.keys():\n",
    "    \n",
    "    maxlen=model_dict[file][0]\n",
    "    dropout=model_dict[file][1]\n",
    "    \n",
    "    print('-'*50,'\\nModel with maxlen of {} and dropout of {}'.format(maxlen,dropout))\n",
    "    \n",
    "    print('padding maxlen=',maxlen)\n",
    "    #x_train_pad = sequence.pad_sequences(list_tokenized_train, maxlen=model_dict[file])\n",
    "    x_test_pad = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "\n",
    "    print('loading model')\n",
    "    loaded_model=load_model(file)\n",
    "    \n",
    "    print('compiling model')\n",
    "    loaded_model.compile(loss='binary_crossentropy',optimizer='adam',metrics=[categorical_accuracy])\n",
    "    print('evaluating model')\n",
    "    score = loaded_model.evaluate(x_test_pad, y_test, verbose=1)\n",
    "    print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))\n",
    "    print('-'*50,'\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing data...\n",
      "Splitting data...\n",
      "127656 train sequences\n",
      "31915 test sequences\n",
      "padding maxlen= 150\n",
      "building model\n",
      "fitting model with max_features=10000, maxlen = 150, and dropout = 0.1\n",
      "Train on 114890 samples, validate on 12766 samples\n",
      "Epoch 1/4\n",
      "114890/114890 [==============================] - 1488s 13ms/step - loss: 0.0701 - categorical_accuracy: 0.9329 - val_loss: 0.0488 - val_categorical_accuracy: 0.9939\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04875, saving model to weights_base_10000_150_0.1.hdf5\n",
      "Epoch 2/4\n",
      "114890/114890 [==============================] - 1410s 12ms/step - loss: 0.0482 - categorical_accuracy: 0.9766 - val_loss: 0.0470 - val_categorical_accuracy: 0.9934\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.04875 to 0.04698, saving model to weights_base_10000_150_0.1.hdf5\n",
      "Epoch 3/4\n",
      "114890/114890 [==============================] - 1154s 10ms/step - loss: 0.0426 - categorical_accuracy: 0.9715 - val_loss: 0.0461 - val_categorical_accuracy: 0.9934\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.04698 to 0.04615, saving model to weights_base_10000_150_0.1.hdf5\n",
      "Epoch 4/4\n",
      "114890/114890 [==============================] - 1153s 10ms/step - loss: 0.0374 - categorical_accuracy: 0.9716 - val_loss: 0.0528 - val_categorical_accuracy: 0.9929\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n"
     ]
    }
   ],
   "source": [
    "# import data\n",
    "print('Importing data...')\n",
    "train_all=pd.read_csv('train.csv')\n",
    "\n",
    "# 80/20 split\n",
    "train_80, test_20 = model_selection.train_test_split(train_all,test_size=0.2)\n",
    "\n",
    "#preprocessing\n",
    "\n",
    "print('Splitting data...')\n",
    "\n",
    "#training data\n",
    "x_train = train_80['comment_text'].fillna(\"nada\").values\n",
    "y_train = train_80[['toxic','severe_toxic','obscene','threat','insult','identity_hate']].values\n",
    "\n",
    "#testing validation data (not for training model, just validation for )\n",
    "\n",
    "x_test = test_20['comment_text'].fillna(\"nada\").values\n",
    "y_test = test_20[['toxic','severe_toxic','obscene','threat','insult','identity_hate']].values\n",
    "\n",
    "# check lengths\n",
    "\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "\n",
    "\n",
    "# MODEL PARAMETERS\n",
    "\n",
    "max_feature_list=[20000,10000]\n",
    "EPOCHS=4\n",
    "maxlen_list=[150,250] \n",
    "dropout_list=[0.1,0.2]\n",
    "\n",
    "#TOKENIZE \n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=max_feature_list[1])\n",
    "tokenizer.fit_on_texts(list(train_all[\"comment_text\"].fillna(\"nada\").values)) #fit on all comment_text\n",
    "\n",
    "#create tokenized comments\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(x_train)  #training (80% of train.csv)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(x_test)  #testing (20% of train.csv)\n",
    "\n",
    "\n",
    "\n",
    "for max_feature in [max_feature_list[1]]:\n",
    "    \n",
    "    for maxlen in [maxlen_list[0]]:\n",
    "        \n",
    "        for dropout in [dropout_list[0]]:\n",
    "            \n",
    "            print('padding maxlen=',maxlen)\n",
    "            x_train_pad = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "            x_test_pad = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "            \n",
    "            print('building model')\n",
    "            model=get_model(maxlen=maxlen,max_features=max_feature,dropout=dropout,embed_size=128)\n",
    "            \n",
    "            batch_size = 32\n",
    "\n",
    "\n",
    "            weight_file_path=\"weights_base_{}_{}_{}.hdf5\".format(max_feature,maxlen,dropout)\n",
    "            model_file_path='bidirectional_lstm_globMP_relu_sigmoid_maxfeat{}_maxlen{}_dropout{}'.format(max_feature,maxlen,dropout)\n",
    "\n",
    "            checkpoint = ModelCheckpoint(weight_file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "            early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=20)\n",
    "\n",
    "            callbacks_list = [checkpoint, early] #early\n",
    "            \n",
    "            #fit model\n",
    "            print('fitting model with max_features={}, maxlen = {}, and dropout = {}'.format(max_feature,maxlen,dropout))\n",
    "            model.fit(x_train_pad, y_train, batch_size=batch_size, epochs=EPOCHS, validation_split=0.1, callbacks=callbacks_list)\n",
    "            \n",
    "            #save model\n",
    "            model.save(model_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compiling model\n",
      "evaluating model\n",
      "31915/31915 [==============================] - 78s 2ms/step\n",
      "categorical_accuracy: 99.32%\n",
      "-------------------------------------------------- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "loaded_model=model\n",
    "\n",
    "x_test_pad = sequence.pad_sequences(list_tokenized_test, maxlen=150)\n",
    "\n",
    "print('compiling model')\n",
    "loaded_model.compile(loss='binary_crossentropy',optimizer='adam',metrics=[categorical_accuracy])\n",
    "print('evaluating model')\n",
    "score = loaded_model.evaluate(x_test_pad, y_test, verbose=1)\n",
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))\n",
    "print('-'*50,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict=model.predict(x_test_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict=pd.DataFrame(y_predict)\n",
    "y_test=pd.DataFrame(y_test)\n",
    "\n",
    "y_predict.columns=['toxic','severe_toxic','obscene','threat','insult','identity_hate']\n",
    "y_test.columns=['toxic','severe_toxic','obscene','threat','insult','identity_hate']\n",
    "\n",
    "\n",
    "# get predictions with decision boundary = 0.5\n",
    "for c in [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]:\n",
    "\n",
    "    y_predict[c]=y_predict[c].map(lambda x: 1 if x >=0.5 else 0)\n",
    "    \n",
    "# subract predictions from true labels to get type I (= -1) and type II (=1)\n",
    "y_error=y_test-y_predict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    toxic  severe_toxic  obscene  threat  insult  identity_hate\n",
       "0       0             0        0       0       0              0\n",
       "1       0             0        0       0       0              0\n",
       "2       0             0        0       0       0              0\n",
       "3       0             0        0       0       0              0\n",
       "4       0             0        0       0       0              0\n",
       "5       0             0        0       0       0              0\n",
       "6       0             0        0       0       0              0\n",
       "7       0             0        0       0       0              0\n",
       "8       0             0        0       0       0              0\n",
       "9       0             0        0       0       0              0\n",
       "10      0             0        0       0       0              0\n",
       "11      0             0        0       0       0              0\n",
       "12      0             0        0       0       0              0\n",
       "13      0             0        0       0       0              0\n",
       "14      0             0        0       0       0              0\n",
       "15      0             0        0       0       0              0\n",
       "16      0             0        0       0       0              0\n",
       "17      0             0        0       0       0              0\n",
       "18      1             0        0       0       0              0\n",
       "19      0             0        0       0       0              0"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_error.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"\\n\\nMay Allah (swt) either give you punishment or hidiyaat for spreading falsehood and keeping other Muslim editors from making the article neutral - Insh\\'Allah.  —Preceding unsigned comment added by 89.108.24.87     —Preceding unsigned comment added by 213.146.172.146   \\n\\nYour comment on your edit proves you are ignorant of Islam.  —Preceding unsigned comment added by 213.146.172.146   \"'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_20['comment_text'].iloc[18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic\n",
      " 0    30690\n",
      " 1      992\n",
      "-1      233\n",
      "Name: toxic, dtype: int64\n",
      "\n",
      " Type I (flagged when no flag exists): 0.73%\n",
      "Type II (not flagged when flag should exist): 3.11%\n",
      "\n",
      " \n",
      "\n",
      "severe_toxic\n",
      " 0    31611\n",
      " 1      187\n",
      "-1      117\n",
      "Name: severe_toxic, dtype: int64\n",
      "\n",
      " Type I (flagged when no flag exists): 0.37%\n",
      "Type II (not flagged when flag should exist): 0.59%\n",
      "\n",
      " \n",
      "\n",
      "obscene\n",
      " 0    31292\n",
      " 1      384\n",
      "-1      239\n",
      "Name: obscene, dtype: int64\n",
      "\n",
      " Type I (flagged when no flag exists): 0.75%\n",
      "Type II (not flagged when flag should exist): 1.20%\n",
      "\n",
      " \n",
      "\n",
      "threat\n",
      "0    31829\n",
      "1       86\n",
      "Name: threat, dtype: int64\n",
      "\n",
      " Type I (flagged when no flag exists): 0.00%\n",
      "Type II (not flagged when flag should exist): 0.27%\n",
      "\n",
      " \n",
      "\n",
      "insult\n",
      " 0    31060\n",
      " 1      532\n",
      "-1      323\n",
      "Name: insult, dtype: int64\n",
      "\n",
      " Type I (flagged when no flag exists): 1.01%\n",
      "Type II (not flagged when flag should exist): 1.67%\n",
      "\n",
      " \n",
      "\n",
      "identity_hate\n",
      " 0    31675\n",
      " 1      161\n",
      "-1       79\n",
      "Name: identity_hate, dtype: int64\n",
      "\n",
      " Type I (flagged when no flag exists): 0.25%\n",
      "Type II (not flagged when flag should exist): 0.50%\n",
      "\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "tot=y_error.shape[0]\n",
    "for c in [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]:\n",
    "    print(c)\n",
    "    print(y_error[c].value_counts())\n",
    "    type1=0\n",
    "    type2=0\n",
    "    \n",
    "    try:\n",
    "        type1=y_error[c].value_counts()[-1]\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        type2=y_error[c].value_counts()[1]\n",
    "    except:\n",
    "        pass\n",
    "    print('\\n Type I (flagged when no flag exists):','{:02.2f}%'.format(type1*100/tot))\n",
    "    print('Type II (not flagged when flag should exist):',\"{:02.2f}%\".format(type2*100/tot))\n",
    "    print('\\n \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31915, 8)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_20.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'dense_11/kernel:0' shape=(100, 50) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "model.weights[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07773774087419709\n"
     ]
    }
   ],
   "source": [
    "\n",
    "wrong=0\n",
    "for index, row in y_error.iterrows():\n",
    "    if y_error.iloc[index].value_counts()[0]!=6:\n",
    "        wrong+=1\n",
    "        \n",
    "print(wrong/y_error.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_error.iloc[1].value_counts()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3333"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([992,233,187,117,384,239,86,532,323,161,79])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.012956290145699515"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.07773774087419709/6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOW LETS AUTOMATE MAX FEATURES 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_feature_list=[20000,10000]\n",
    "EPOCHS=4\n",
    "maxlen_list=[150,250] \n",
    "dropout_list=[0.1,0.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=max_feature_list[1])\n",
    "\n",
    "tokenizer.fit_on_texts(list(train_all[\"comment_text\"].fillna(\"nada\").values)) #fit on all comment_text\n",
    "\n",
    "#create tokenized comments\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(x_train)  #training (80% of train.csv)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(x_test)  #testing (20% of train.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "padding maxlen= 150\n",
      "building model\n",
      "fitting model with max_features=10000, maxlen = 150, and dropout = 0.2\n",
      "Train on 114890 samples, validate on 12766 samples\n",
      "Epoch 1/4\n",
      "114890/114890 [==============================] - 1208s 11ms/step - loss: 0.0695 - categorical_accuracy: 0.9150 - val_loss: 0.0489 - val_categorical_accuracy: 0.9940\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04887, saving model to weights_base_10000_150_0.2.hdf5\n",
      "Epoch 2/4\n",
      "114890/114890 [==============================] - 1168s 10ms/step - loss: 0.0488 - categorical_accuracy: 0.9832 - val_loss: 0.0475 - val_categorical_accuracy: 0.9940\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.04887 to 0.04755, saving model to weights_base_10000_150_0.2.hdf5\n",
      "Epoch 3/4\n",
      "114890/114890 [==============================] - 1158s 10ms/step - loss: 0.0437 - categorical_accuracy: 0.9822 - val_loss: 0.0469 - val_categorical_accuracy: 0.9938\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.04755 to 0.04693, saving model to weights_base_10000_150_0.2.hdf5\n",
      "Epoch 4/4\n",
      "114890/114890 [==============================] - 1153s 10ms/step - loss: 0.0392 - categorical_accuracy: 0.9637 - val_loss: 0.0507 - val_categorical_accuracy: 0.9940\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "padding maxlen= 250\n",
      "building model\n",
      "fitting model with max_features=10000, maxlen = 250, and dropout = 0.1\n",
      "Train on 114890 samples, validate on 12766 samples\n",
      "Epoch 1/4\n",
      "114890/114890 [==============================] - 1877s 16ms/step - loss: 0.0671 - categorical_accuracy: 0.9216 - val_loss: 0.0493 - val_categorical_accuracy: 0.9934\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04929, saving model to weights_base_10000_250_0.1.hdf5\n",
      "Epoch 2/4\n",
      "114890/114890 [==============================] - 1819s 16ms/step - loss: 0.0476 - categorical_accuracy: 0.9732 - val_loss: 0.0471 - val_categorical_accuracy: 0.9925\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.04929 to 0.04713, saving model to weights_base_10000_250_0.1.hdf5\n",
      "Epoch 3/4\n",
      "114890/114890 [==============================] - 1807s 16ms/step - loss: 0.0423 - categorical_accuracy: 0.9692 - val_loss: 0.0484 - val_categorical_accuracy: 0.9940\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/4\n",
      "114890/114890 [==============================] - 1790s 16ms/step - loss: 0.0373 - categorical_accuracy: 0.9480 - val_loss: 0.0481 - val_categorical_accuracy: 0.9940\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "padding maxlen= 250\n",
      "building model\n",
      "fitting model with max_features=10000, maxlen = 250, and dropout = 0.2\n",
      "Train on 114890 samples, validate on 12766 samples\n",
      "Epoch 1/4\n",
      "114890/114890 [==============================] - 1809s 16ms/step - loss: 0.0779 - categorical_accuracy: 0.9109 - val_loss: 0.0484 - val_categorical_accuracy: 0.9940\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04844, saving model to weights_base_10000_250_0.2.hdf5\n",
      "Epoch 2/4\n",
      "114890/114890 [==============================] - 1811s 16ms/step - loss: 0.0495 - categorical_accuracy: 0.9872 - val_loss: 0.0476 - val_categorical_accuracy: 0.9940\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.04844 to 0.04760, saving model to weights_base_10000_250_0.2.hdf5\n",
      "Epoch 3/4\n",
      "114890/114890 [==============================] - 1823s 16ms/step - loss: 0.0443 - categorical_accuracy: 0.9839 - val_loss: 0.0466 - val_categorical_accuracy: 0.9936\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.04760 to 0.04663, saving model to weights_base_10000_250_0.2.hdf5\n",
      "Epoch 4/4\n",
      "114890/114890 [==============================] - 1807s 16ms/step - loss: 0.0395 - categorical_accuracy: 0.9665 - val_loss: 0.0485 - val_categorical_accuracy: 0.9918\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n"
     ]
    }
   ],
   "source": [
    "max_feature_list=[20000,10000]\n",
    "maxlen_list=[150,250] \n",
    "dropout_list=[0.1,0.2]\n",
    "EPOCHS=4\n",
    "\n",
    "for max_feature in [max_feature_list[1]]:\n",
    "    \n",
    "    for maxlen in maxlen_list:\n",
    "        \n",
    "        for dropout in dropout_list:\n",
    "            \n",
    "            if (maxlen,dropout) !=(150,0.1):  # already did 150,0.1\n",
    "                print('padding maxlen=',maxlen)\n",
    "                x_train_pad = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "                x_test_pad = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "\n",
    "                print('building model')\n",
    "                model=get_model(maxlen=maxlen,max_features=max_feature,dropout=dropout,embed_size=128)\n",
    "\n",
    "                batch_size = 32\n",
    "\n",
    "\n",
    "                weight_file_path=\"weights_base_{}_{}_{}.hdf5\".format(max_feature,maxlen,dropout)\n",
    "                model_file_path='bidirectional_lstm_globMP_relu_sigmoid_maxfeat{}_maxlen{}_dropout{}'.format(max_feature,maxlen,dropout)\n",
    "\n",
    "                checkpoint = ModelCheckpoint(weight_file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "                early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=20)\n",
    "\n",
    "                callbacks_list = [checkpoint, early] #early\n",
    "\n",
    "                #fit model\n",
    "                print('fitting model with max_features={}, maxlen = {}, and dropout = {}'.format(max_feature,maxlen,dropout))\n",
    "                model.fit(x_train_pad, y_train, batch_size=batch_size, epochs=EPOCHS, validation_split=0.1, callbacks=callbacks_list)\n",
    "\n",
    "                #save model\n",
    "                model.save(model_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 150 0.2\n",
      "10000 250 0.1\n",
      "10000 250 0.2\n"
     ]
    }
   ],
   "source": [
    "max_feature_list=[20000,10000]\n",
    "maxlen_list=[150,250] \n",
    "dropout_list=[0.1,0.2]\n",
    "EPOCHS=4\n",
    "\n",
    "for max_feature in [max_feature_list[1]]:\n",
    "    \n",
    "    for maxlen in maxlen_list:\n",
    "        \n",
    "        for dropout in dropout_list:\n",
    "            \n",
    "            if (maxlen,dropout) !=(150,0.1):\n",
    "                \n",
    "                print(max_feature,maxlen,dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict={'bidirectional_lstm_globMP_relu_sigmoid_maxfeat10000_maxlen150_dropout0.1':[150,0.1],\\\n",
    "'bidirectional_lstm_globMP_relu_sigmoid_maxfeat10000_maxlen150_dropout0.2':[150,0.2],\\\n",
    "'bidirectional_lstm_globMP_relu_sigmoid_maxfeat10000_maxlen250_dropout0.1':[250,0.1],\\\n",
    "'bidirectional_lstm_globMP_relu_sigmoid_maxfeat10000_maxlen250_dropout0.2':[250,0.2]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------- \n",
      "Model with max_features 10000, maxlen of 150, and dropout of 0.1\n",
      "padding maxlen= 150\n",
      "loading model\n",
      "compiling model\n",
      "evaluating model\n",
      "31915/31915 [==============================] - 84s 3ms/step\n",
      "categorical_accuracy: 99.27%\n",
      "-------------------------------------------------- \n",
      "\n",
      "-------------------------------------------------- \n",
      "Model with max_features 10000, maxlen of 150, and dropout of 0.2\n",
      "padding maxlen= 150\n",
      "loading model\n",
      "compiling model\n",
      "evaluating model\n",
      "31915/31915 [==============================] - 80s 3ms/step\n",
      "categorical_accuracy: 99.35%\n",
      "-------------------------------------------------- \n",
      "\n",
      "-------------------------------------------------- \n",
      "Model with max_features 10000, maxlen of 250, and dropout of 0.1\n",
      "padding maxlen= 250\n",
      "loading model\n",
      "compiling model\n",
      "evaluating model\n",
      "31915/31915 [==============================] - 126s 4ms/step\n",
      "categorical_accuracy: 99.35%\n",
      "-------------------------------------------------- \n",
      "\n",
      "-------------------------------------------------- \n",
      "Model with max_features 10000, maxlen of 250, and dropout of 0.2\n",
      "padding maxlen= 250\n",
      "loading model\n",
      "compiling model\n",
      "evaluating model\n",
      "31915/31915 [==============================] - 139s 4ms/step\n",
      "categorical_accuracy: 99.15%\n",
      "-------------------------------------------------- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model \n",
    "\n",
    "# Returns a compiled model identical to the previous one\n",
    "#model = load_model(\"model_name.hdf5\")\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=10000)\n",
    "#tokenize based on comment database\n",
    "tokenizer.fit_on_texts(list(train_all[\"comment_text\"].fillna(\"nada\").values)) #fit on all comment_text\n",
    "#create tokenized comments\n",
    "#list_tokenized_train = tokenizer.texts_to_sequences(x_train)  #training (80% of train.csv)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(x_test)  #testing (20% of train.csv)\n",
    "\n",
    "for file in model_dict.keys():\n",
    "    \n",
    "    maxlen=model_dict[file][0]\n",
    "    dropout=model_dict[file][1]\n",
    "    \n",
    "    print('-'*50,'\\nModel with max_features 10000, maxlen of {}, and dropout of {}'.format(maxlen,dropout))\n",
    "    \n",
    "    print('padding maxlen=',maxlen)\n",
    "    #x_train_pad = sequence.pad_sequences(list_tokenized_train, maxlen=model_dict[file])\n",
    "    x_test_pad = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "\n",
    "    print('loading model')\n",
    "    loaded_model=load_model(file)\n",
    "    \n",
    "    print('compiling model')\n",
    "    loaded_model.compile(loss='binary_crossentropy',optimizer='adam',metrics=[categorical_accuracy])\n",
    "    print('evaluating model')\n",
    "    score = loaded_model.evaluate(x_test_pad, y_test, verbose=1)\n",
    "    print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))\n",
    "    print('-'*50,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.topology.InputLayer at 0x16d47ebe0>,\n",
       " <keras.layers.embeddings.Embedding at 0x168198b38>,\n",
       " <keras.layers.wrappers.Bidirectional at 0x174c4cbe0>,\n",
       " <keras.layers.pooling.GlobalMaxPooling1D at 0x174c2bfd0>,\n",
       " <keras.layers.core.Dropout at 0x174c2bf98>,\n",
       " <keras.layers.core.Dense at 0x174c2bf60>,\n",
       " <keras.layers.core.Dropout at 0x16d08dd68>,\n",
       " <keras.layers.core.Dense at 0x174cc8da0>]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model\n",
      "compiling model\n",
      "evaluating model\n",
      "31915/31915 [==============================] - 129s 4ms/step\n",
      "binary_accuracy: 98.31%\n",
      "-------------------------------------------------- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('loading model')\n",
    "loaded_model=load_model('bidirectional_lstm_globMP_relu_sigmoid_maxfeat10000_maxlen250_dropout0.2')\n",
    "\n",
    "print('compiling model')\n",
    "loaded_model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['binary_accuracy','categorical_accuracy'])\n",
    "print('evaluating model')\n",
    "score = loaded_model.evaluate(x_test_pad, y_test, verbose=1)\n",
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[:], score[1]*100))\n",
    "print('-'*50,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 4.92%\n",
      "binary_accuracy: 98.31%\n",
      "categorical_accuracy: 99.15%\n",
      "sparse_categorical_accuracy: 89.87%\n"
     ]
    }
   ],
   "source": [
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[0], score[0]*100))\n",
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))\n",
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[2], score[2]*100))\n",
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[3], score[3]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AUTOMATE ACCURACY TEST ON NON-MAXPOOLED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model_dict={'bidirectional_lstm_relu_sigmoid_maxfeat10000_maxlen150_dropout0.1':[10000,150],\\\n",
    "                 'bidirectional_lstm_relu_sigmoid_maxfeat10000_maxlen250_dropout0.1':[10000,250],\\\n",
    "                'bidirectional_lstm_relu_sigmoid_maxfeat20000_maxlen150_dropout0.1':[20000,150],\\\n",
    "                'bidirectional_lstm_relu_sigmoid_maxfeat20000_maxlen250_dropout0.1':[20000,250],}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "#list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_t = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159571,)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"train.csv\")\n",
    "list_sentences_train_all = train[\"comment_text\"].fillna(\"nada\").values\n",
    "list_sentences_train_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>He just MIGHT have been telling the truth but ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"\\nNo, it's no, and I know it. I also know (an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Kurt Cobain\\nwhy do you think you own the Kurt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Edit request \\n\\nThank you for , can you do th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AlphaChump \\n\\nAttempting to erase history is ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  He just MIGHT have been telling the truth but ...\n",
       "1  \"\\nNo, it's no, and I know it. I also know (an...\n",
       "2  Kurt Cobain\\nwhy do you think you own the Kurt...\n",
       "3  Edit request \\n\\nThank you for , can you do th...\n",
       "4  AlphaChump \\n\\nAttempting to erase history is ..."
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test=pd.read_csv('x_test.csv',header=-1)\n",
    "x_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build before running\n",
    "\n",
    "performance_dict=dict()\n",
    "\n",
    "for key in test_model_dict.keys():\n",
    "    performance_dict[key]={'total_toxic':0,'toxic_falsepos':0,'toxic_falseneg':0,\\\n",
    "                           'toxic_falsepos_comment':[],'toxic_falseneg_comment':[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bidirectional_lstm_relu_sigmoid_maxfeat10000_maxlen150_dropout0.1\n",
      "loading model\n",
      "tokenizing\n",
      "compiling model\n",
      "evaluating model\n",
      "31915/31915 [==============================] - 77s 2ms/step\n",
      "categorical_accuracy: 99.39%\n",
      "creating prediction\n",
      "31915/31915 [==============================] - 75s 2ms/step\n",
      "test predictions on \"toxic\" category\n",
      "total true toxic 2933\n",
      "total false positive (incorrectly flagged as toxic) Type I 744\n",
      "false positive rate 25.367 %\n",
      "total false negative (omitted flagging as toxic) Type II 670\n",
      "false negative rate 22.844 %\n",
      "-------------------------------------------------- \n",
      "\n",
      "bidirectional_lstm_relu_sigmoid_maxfeat10000_maxlen250_dropout0.1\n",
      "loading model\n",
      "tokenizing\n",
      "compiling model\n",
      "evaluating model\n",
      "31915/31915 [==============================] - 115s 4ms/step\n",
      "categorical_accuracy: 98.50%\n",
      "creating prediction\n",
      "31915/31915 [==============================] - 114s 4ms/step\n",
      "test predictions on \"toxic\" category\n",
      "total true toxic 2933\n",
      "total false positive (incorrectly flagged as toxic) Type I 616\n",
      "false positive rate 21.002 %\n",
      "total false negative (omitted flagging as toxic) Type II 669\n",
      "false negative rate 22.809 %\n",
      "-------------------------------------------------- \n",
      "\n",
      "bidirectional_lstm_relu_sigmoid_maxfeat20000_maxlen150_dropout0.1\n",
      "loading model\n",
      "tokenizing\n",
      "compiling model\n",
      "evaluating model\n",
      "31915/31915 [==============================] - 86s 3ms/step\n",
      "categorical_accuracy: 99.36%\n",
      "creating prediction\n",
      "31915/31915 [==============================] - 78s 2ms/step\n",
      "test predictions on \"toxic\" category\n",
      "total true toxic 2933\n",
      "total false positive (incorrectly flagged as toxic) Type I 222\n",
      "false positive rate 7.569 %\n",
      "total false negative (omitted flagging as toxic) Type II 670\n",
      "false negative rate 22.844 %\n",
      "-------------------------------------------------- \n",
      "\n",
      "bidirectional_lstm_relu_sigmoid_maxfeat20000_maxlen250_dropout0.1\n",
      "loading model\n",
      "tokenizing\n",
      "compiling model\n",
      "evaluating model\n",
      "31915/31915 [==============================] - 112s 4ms/step\n",
      "categorical_accuracy: 99.40%\n",
      "creating prediction\n",
      "31915/31915 [==============================] - 106s 3ms/step\n",
      "test predictions on \"toxic\" category\n",
      "total true toxic 2933\n",
      "total false positive (incorrectly flagged as toxic) Type I 432\n",
      "false positive rate 14.729 %\n",
      "total false negative (omitted flagging as toxic) Type II 326\n",
      "false negative rate 11.115 %\n",
      "-------------------------------------------------- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "\n",
    "test_model_dict={'bidirectional_lstm_relu_sigmoid_maxfeat10000_maxlen150_dropout0.1':[10000,150],\\\n",
    "                 'bidirectional_lstm_relu_sigmoid_maxfeat10000_maxlen250_dropout0.1':[10000,250],\\\n",
    "                'bidirectional_lstm_relu_sigmoid_maxfeat20000_maxlen150_dropout0.1':[20000,150],\\\n",
    "                'bidirectional_lstm_relu_sigmoid_maxfeat20000_maxlen250_dropout0.1':[20000,250],}\n",
    "\n",
    "performance_dict=dict()\n",
    "\n",
    "for key in test_model_dict.keys():\n",
    "    performance_dict[key]={'total_toxic':0,'toxic_falsepos':0,'toxic_falseneg':0,\\\n",
    "                           'toxic_falsepos_comment':[],'toxic_falseneg_comment':[]}\n",
    "\n",
    "#create correct classification array keep as DataFrame\n",
    "gold=pd.read_csv('gold.csv')\n",
    "gold.columns=list_classes\n",
    "\n",
    "#load test data and convert to np.array\n",
    "x_test=pd.read_csv('x_test.csv',header=-1)\n",
    "x_test=x_test[0].fillna(\"nada\").values\n",
    "\n",
    "for key in test_model_dict.keys():\n",
    "    \n",
    "    print(key)\n",
    "    # load model\n",
    "    print('loading model')\n",
    "    test_model=load_model(key)\n",
    "    \n",
    "    # tokenize x_test\n",
    "    print('tokenizing')\n",
    "    max_features=test_model_dict[key][0] #max features for model\n",
    "    maxlen=test_model_dict[key][1] #max sequence length for model\n",
    "    \n",
    "    tokenizer = text.Tokenizer(num_words=max_features)    # set up tokenizer\n",
    "    tokenizer.fit_on_texts(list(list_sentences_train_all)) # fit tokenizer to all in train.csv\n",
    "    tokenized_x_test = tokenizer.texts_to_sequences(x_test) # tokenize x_test\n",
    "    test_sequence=sequence.pad_sequences(tokenized_x_test, maxlen=maxlen) #create the test sequence\n",
    "    \n",
    "    # \n",
    "    print('compiling model')\n",
    "    test_model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=[categorical_accuracy])\n",
    "    \n",
    "    print('evaluating model')\n",
    "    score = test_model.evaluate(test_sequence, y_test, verbose=1)\n",
    "    \n",
    "    print(\"%s: %.2f%%\" % (test_model.metrics_names[1], score[1]*100))\n",
    "    \n",
    "    print('creating prediction')\n",
    "    predict=test_model.predict(test_sequence, verbose=1)\n",
    "    \n",
    "    print('test predictions on \"toxic\" category')\n",
    "    predict_df=pd.DataFrame(predict)\n",
    "    predict_df.columns=list_classes\n",
    "    \n",
    "    predict_df.to_csv('prediction_'+key+'.csv')\n",
    "    \n",
    "    for c in predict_df.columns:\n",
    "         predict_df[c]=predict_df[c].map(lambda x: 1 if x >=0.5 else 0)\n",
    "\n",
    "    err_df=gold['toxic']-predict_df['toxic']  #val of -1 is false positive Type I, value of +1 is false negative Type II\n",
    "    \n",
    "    performance_dict[key]['total_toxic']=sum(gold['toxic'])\n",
    "    performance_dict[key]['toxic_falsepos']=err_df.value_counts()[-1]\n",
    "    performance_dict[key]['toxic_falseneg']=err_df.value_counts()[1]\n",
    "    \n",
    "    print('total true toxic',sum(gold['toxic']))\n",
    "    #false positives\n",
    "    print('total false positive (incorrectly flagged as toxic) Type I',err_df.value_counts()[-1])\n",
    "    print('false positive rate', '{:02.3f}'.format(100*err_df.value_counts()[-1]/sum(gold['toxic'])),'%')\n",
    "    #false negatives\n",
    "    print('total false negative (omitted flagging as toxic) Type II',err_df.value_counts()[1])\n",
    "    print('false negative rate', '{:02.3f}'.format(100*err_df.value_counts()[1]/sum(gold['toxic'])),'%')\n",
    "    \n",
    "    \n",
    "    print('-'*50,'\\n')\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bidirectional_lstm_relu_sigmoid_maxfeat10000_maxlen150_dropout0.1\n",
    "loading model\n",
    "tokenizing\n",
    "compiling model\n",
    "evaluating model\n",
    "31915/31915 [==============================] - 77s 2ms/step\n",
    "\n",
    "categorical_accuracy: 99.39%\n",
    "\n",
    "creating prediction\n",
    "\n",
    "31915/31915 [==============================] - 75s 2ms/step\n",
    "\n",
    "test predictions on \"toxic\" category\n",
    "\n",
    "total true toxic 2933\n",
    "\n",
    "total false positive (incorrectly flagged as toxic) Type I 744\n",
    "\n",
    "false positive rate 25.367 %\n",
    "\n",
    "total false negative (omitted flagging as toxic) Type II 670\n",
    "\n",
    "false negative rate 22.844 %\n",
    "\n",
    "\n",
    "-------------------------------------------------- \n",
    "\n",
    "bidirectional_lstm_relu_sigmoid_maxfeat10000_maxlen250_dropout0.1\n",
    "\n",
    "loading model\n",
    "\n",
    "tokenizing\n",
    "\n",
    "compiling model\n",
    "\n",
    "evaluating model\n",
    "\n",
    "31915/31915 [==============================] - 115s 4ms/step\n",
    "\n",
    "categorical_accuracy: 98.50%\n",
    "\n",
    "creating prediction\n",
    "\n",
    "31915/31915 [==============================] - 114s 4ms/step\n",
    "\n",
    "test predictions on \"toxic\" category\n",
    "\n",
    "total true toxic 2933\n",
    "\n",
    "total false positive (incorrectly flagged as toxic) Type I 616\n",
    "\n",
    "false positive rate 21.002 %\n",
    "\n",
    "total false negative (omitted flagging as toxic) Type II 669\n",
    "\n",
    "false negative rate 22.809 %\n",
    "\n",
    "-------------------------------------------------- \n",
    "\n",
    "bidirectional_lstm_relu_sigmoid_maxfeat20000_maxlen150_dropout0.1\n",
    "\n",
    "loading model\n",
    "\n",
    "tokenizing\n",
    "\n",
    "compiling model\n",
    "\n",
    "evaluating model\n",
    "\n",
    "31915/31915 [==============================] - 86s 3ms/step\n",
    "\n",
    "categorical_accuracy: 99.36%\n",
    "\n",
    "creating prediction\n",
    "\n",
    "31915/31915 [==============================] - 78s 2ms/step\n",
    "\n",
    "test predictions on \"toxic\" category\n",
    "\n",
    "total true toxic 2933\n",
    "\n",
    "total false positive (incorrectly flagged as toxic) Type I 222\n",
    "\n",
    "false positive rate 7.569 %\n",
    "\n",
    "total false negative (omitted flagging as toxic) Type II 670\n",
    "\n",
    "false negative rate 22.844 %\n",
    "\n",
    "-------------------------------------------------- \n",
    "\n",
    "bidirectional_lstm_relu_sigmoid_maxfeat20000_maxlen250_dropout0.1\n",
    "\n",
    "loading model\n",
    "\n",
    "tokenizing\n",
    "\n",
    "compiling model\n",
    "\n",
    "evaluating model\n",
    "\n",
    "31915/31915 [==============================] - 112s 4ms/step\n",
    "\n",
    "categorical_accuracy: 99.40%\n",
    "\n",
    "creating prediction\n",
    "\n",
    "31915/31915 [==============================] - 106s 3ms/step\n",
    "\n",
    "test predictions on \"toxic\" category\n",
    "\n",
    "total true toxic 2933\n",
    "\n",
    "total false positive (incorrectly flagged as toxic) Type I 432\n",
    "\n",
    "false positive rate 14.729 %\n",
    "\n",
    "total false negative (omitted flagging as toxic) Type II 326\n",
    "\n",
    "false negative rate 11.115 %\n",
    "\n",
    "-------------------------------------------------- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bidirectional_lstm_relu_sigmoid_maxfeat10000_maxlen150_dropout0.1\n",
    "loading model\n",
    "tokenizing\n",
    "compiling model\n",
    "evaluating model\n",
    "31915/31915 [==============================] - 73s 2ms/step\n",
    "categorical_accuracy: 99.39%\n",
    "-------------------------------------------------- \n",
    "\n",
    "bidirectional_lstm_relu_sigmoid_maxfeat10000_maxlen250_dropout0.1\n",
    "loading model\n",
    "tokenizing\n",
    "compiling model\n",
    "evaluating model\n",
    "31915/31915 [==============================] - 113s 4ms/step\n",
    "categorical_accuracy: 98.50%\n",
    "-------------------------------------------------- \n",
    "\n",
    "bidirectional_lstm_relu_sigmoid_maxfeat20000_maxlen150_dropout0.1\n",
    "loading model\n",
    "tokenizing\n",
    "compiling model\n",
    "evaluating model\n",
    "31915/31915 [==============================] - 74s 2ms/step\n",
    "categorical_accuracy: 99.36%\n",
    "-------------------------------------------------- \n",
    "\n",
    "bidirectional_lstm_relu_sigmoid_maxfeat20000_maxlen250_dropout0.1\n",
    "loading model\n",
    "tokenizing\n",
    "compiling model\n",
    "evaluating model\n",
    "31915/31915 [==============================] - 116s 4ms/step\n",
    "categorical_accuracy: 99.40%\n",
    "-------------------------------------------------- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    0\n",
       "3    0\n",
       "4    0\n",
       "5    0\n",
       "6    1\n",
       "7    0\n",
       "8    0\n",
       "9    0\n",
       "Name: 0, dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold=pd.DataFrame(y_test)\n",
    "gold[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31915/31915 [==============================] - 102s 3ms/step\n"
     ]
    }
   ],
   "source": [
    "predict=test_model.predict(test_sequence, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_df=pd.DataFrame(predict)\n",
    "\n",
    "for c in predict_df.columns:\n",
    "\n",
    "    predict_df[c]=predict_df[c].map(lambda x: 1 if x >=0.5 else 0)\n",
    "\n",
    "err_df=gold[0]-predict_df[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.729\n"
     ]
    }
   ],
   "source": [
    "print('{:02.3f}'.format(100*err_df.value_counts()[-1]/sum(gold[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['\"\\nNo, it\\'s no, and I know it. I also know (and understand) about WP:FILMPLOT, which is something you appear not to have read. -  (^ • @) \"'],\n",
       "       ['Kurt Cobain\\nwhy do you think you own the Kurt Cobain page? I am not trying to attack you, I am just wondering why.'],\n",
       "       ['Edit request \\n\\nThank you for , can you do the same for  please?'],\n",
       "       ...,\n",
       "       [\"Dictionaries\\n\\nHere are some dictionaries that do not define the word as always offensive: Oxford English Dictionary Online[]\\n'2. A native of China.'\\nMerriam-Webster Online Dictionary[]\\n'often offensive  a native of China  CHINESE '\\nI can find more if needed. I think that these counter-examples can allow the word 'many' to be inserted in front of 'dictionaries' in saying that the word is offensive.\"],\n",
       "       [\"Provided that it's removed entirely to here, on your head be it! ;)\"],\n",
       "       ['fucking me hard because i am an e-geek']], dtype=object)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [1, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create correct classification array\n",
    "\n",
    "\n",
    "gold=pd.read_csv('gold.csv').values\n",
    "gold[:10]\n",
    "\n",
    "#load test data and convert to np.array\n",
    "#x_test=pd.read_csv('x_test.csv')\n",
    "#x_test=x_test.fillna(\"nada\").values\n",
    "#x_text[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in test_model_dict.keys():\n",
    "    performance_dict[key]={'total_toxic':0,'toxic_falsepos':0,'toxic_falseneg':0,\\\n",
    "                           'toxic_falsepos_comment':[],'toxic_falseneg_comment':[]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31915, 1)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dummy= pd.DataFrame(gold['toxic']-gold['threat'])\n",
    "dummy.columns=['error']\n",
    "\n",
    "dummy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'head'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-140-06bc9693864b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'head'"
     ]
    }
   ],
   "source": [
    "x_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>error</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   error    0\n",
       "0    0.0  NaN\n",
       "1    0.0  NaN\n",
       "2    0.0  NaN\n",
       "3    0.0  NaN\n",
       "4    0.0  NaN"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
