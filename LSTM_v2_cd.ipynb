{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "143613/143613 [==============================] - 1069s 7ms/step - loss: 0.2844 - acc: 0.9907 - val_loss: 0.2791 - val_acc: 0.9939\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.27913, saving model to weights_base.best.hdf5\n",
      "Epoch 2/2\n",
      "143613/143613 [==============================] - 1116s 8ms/step - loss: 0.2668 - acc: 0.9926 - val_loss: 0.2764 - val_acc: 0.9896\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.27913 to 0.27637, saving model to weights_base.best.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x128f59c50>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Embedding, Input\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "max_features = 20000\n",
    "maxlen = 100\n",
    "\n",
    "\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "#test = pd.read_csv(\"test.csv\")\n",
    "#train = train.sample(frac=1)\n",
    "\n",
    "list_sentences_train = train[\"comment_text\"].fillna(\"nada\").values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "#list_sentences_test = test[\"comment_text\"].fillna(\"nada\").values\n",
    "\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "#list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_t = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "#X_te = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "\n",
    "def get_model():\n",
    "    embed_size = 128\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size)(inp)\n",
    "    x = Bidirectional(LSTM(50, return_sequences=True))(x)\n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(50, activation=\"relu\")(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(6, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model = get_model()\n",
    "batch_size = 32\n",
    "epochs = 2\n",
    "\n",
    "\n",
    "file_path=\"weights_base.best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=20)\n",
    "\n",
    "\n",
    "callbacks_list = [checkpoint, early] #early\n",
    "model.fit(X_t, y, batch_size=batch_size, epochs=epochs, validation_split=0.1, callbacks=callbacks_list)\n",
    "\n",
    "#model.load_weights(file_path)\n",
    "\n",
    "#y_predict = model.predict(X_te)\n",
    "\n",
    "\n",
    "\n",
    "#sample_submission = pd.read_csv(\"sample_submission.csv\")\n",
    "\n",
    "#sample_submission[list_classes] = y_predict\n",
    "\n",
    "\n",
    "\n",
    "#sample_submission.to_csv(\"baseline.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#maxlens=[100,200,300]\n",
    "#batch_sizes=[32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_len_bin=dict()\n",
    "\n",
    "comment_len_bin['total']=train.shape[0]\n",
    "comment_len_bin['100']=0\n",
    "comment_len_bin['150']=0\n",
    "comment_len_bin['200']=0\n",
    "comment_len_bin['250']=0\n",
    "comment_len_bin['300']=0\n",
    "\n",
    "for index,row in train.iterrows():\n",
    "    \n",
    "    if len(row['comment_text'].split(' '))<=100:\n",
    "        comment_len_bin['100']+=(1)\n",
    "        \n",
    "    if len(row['comment_text'].split(' '))<=150:\n",
    "        comment_len_bin['150']+=(1)\n",
    "\n",
    "    if len(row['comment_text'].split(' '))<=200:\n",
    "        comment_len_bin['200']+=(1)\n",
    "\n",
    "    if len(row['comment_text'].split(' '))<=250:\n",
    "        comment_len_bin['250']+=(1)\n",
    "\n",
    "    if len(row['comment_text'].split(' '))<=300:\n",
    "        comment_len_bin['300']+=(1)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'100': 130994, '150': 0, '200': 0, '250': 0, '300': 0, 'total': 159571}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment_len_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 82.09 % of comments have length equal or less than 100 words\n",
      " 89.78 % of comments have length equal or less than 150 words\n",
      " 93.61 % of comments have length equal or less than 200 words\n",
      " 95.75 % of comments have length equal or less than 250 words\n",
      " 96.95 % of comments have length equal or less than 300 words\n"
     ]
    }
   ],
   "source": [
    "for i in ['100','150','200','250','300']:\n",
    "    print('{:6.2f}'.format(100*comment_len_bin[i]/comment_len_bin['total']),'% of comments have length equal or less than', i, 'words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets Automate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget('https://s3.amazonaws.com/danicic-w266-final/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ChrisD/anaconda3/envs/tensorflow/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from scipy.sparse import hstack\n",
    "from sklearn import model_selection\n",
    "import numpy as np\n",
    "import tensorflow  as tf\n",
    "import keras\n",
    "import h5py\n",
    "from keras.models import load_model\n",
    "\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Embedding, Input\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from keras.metrics import categorical_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing data...\n",
      "Splitting data...\n",
      "127656 train sequences\n",
      "31915 test sequences\n"
     ]
    }
   ],
   "source": [
    "# import data\n",
    "print('Importing data...')\n",
    "train_all=pd.read_csv('train.csv')\n",
    "\n",
    "# 80/20 split\n",
    "train_80, test_20 = model_selection.train_test_split(train_all,test_size=0.2)\n",
    "\n",
    "#preprocessing\n",
    "\n",
    "print('Splitting data...')\n",
    "\n",
    "#training data\n",
    "x_train = train_80['comment_text'].fillna(\"nada\").values\n",
    "y_train = train_80[['toxic','severe_toxic','obscene','threat','insult','identity_hate']].values\n",
    "\n",
    "#testing validation data (not for training model, just validation for )\n",
    "\n",
    "x_test = test_20['comment_text'].fillna(\"nada\").values\n",
    "y_test = test_20[['toxic','severe_toxic','obscene','threat','insult','identity_hate']].values\n",
    "\n",
    "# check lengths\n",
    "\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_feature_list=[20000,10000]\n",
    "EPOCHS=4\n",
    "maxlen_list=[150,250] \n",
    "dropout_list=[0.1,0.2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = text.Tokenizer(num_words=max_feature_list[0])\n",
    "\n",
    "tokenizer.fit_on_texts(list(train_all[\"comment_text\"].fillna(\"nada\").values)) #fit on all comment_text\n",
    "\n",
    "#create tokenized comments\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(x_train)  #training (80% of train.csv)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(x_test)  #testing (20% of train.csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create LSTM model maker function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(maxlen,max_features,dropout,embed_size=128):\n",
    "    \n",
    "    embed_size=embed_size #default to 128\n",
    "    maxlen=maxlen # max length of sequence input\n",
    "    max_features=max_features  # max vocab\n",
    "    dropout=dropout\n",
    "    \n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    \n",
    "    x = Embedding(max_features, embed_size)(inp)\n",
    "    \n",
    "    x = Bidirectional(LSTM(50, return_sequences=True))(x)\n",
    "    \n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    \n",
    "    x = Dropout(0.1)(x)\n",
    "    \n",
    "    x = Dense(50, activation=\"relu\")(x)\n",
    "    \n",
    "    x = Dropout(0.1)(x)\n",
    "    \n",
    "    x = Dense(6, activation=\"sigmoid\")(x)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=[categorical_accuracy])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize and fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "padding maxlen= 150\n",
      "building model\n",
      "fitting model with max_features=20000, maxlen = 150, and dropout = 0.1\n",
      "Train on 114890 samples, validate on 12766 samples\n",
      "Epoch 1/4\n",
      "114890/114890 [==============================] - 1211s 11ms/step - loss: 0.0658 - categorical_accuracy: 0.9377 - val_loss: 0.0499 - val_categorical_accuracy: 0.9939\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04986, saving model to weights_base_20000_150_0.1.hdf5\n",
      "Epoch 2/4\n",
      "114890/114890 [==============================] - 1234s 11ms/step - loss: 0.0444 - categorical_accuracy: 0.9656 - val_loss: 0.0482 - val_categorical_accuracy: 0.9925\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.04986 to 0.04824, saving model to weights_base_20000_150_0.1.hdf5\n",
      "Epoch 3/4\n",
      "114890/114890 [==============================] - 1198s 10ms/step - loss: 0.0380 - categorical_accuracy: 0.9620 - val_loss: 0.0499 - val_categorical_accuracy: 0.9926\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/4\n",
      "114890/114890 [==============================] - 1187s 10ms/step - loss: 0.0328 - categorical_accuracy: 0.9030 - val_loss: 0.0558 - val_categorical_accuracy: 0.9907\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "padding maxlen= 150\n",
      "building model\n",
      "fitting model with max_features=20000, maxlen = 150, and dropout = 0.2\n",
      "Train on 114890 samples, validate on 12766 samples\n",
      "Epoch 1/4\n",
      "114890/114890 [==============================] - 1198s 10ms/step - loss: 0.0658 - categorical_accuracy: 0.9285 - val_loss: 0.0514 - val_categorical_accuracy: 0.9939\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.05144, saving model to weights_base_20000_150_0.2.hdf5\n",
      "Epoch 2/4\n",
      "114890/114890 [==============================] - 1198s 10ms/step - loss: 0.0457 - categorical_accuracy: 0.9861 - val_loss: 0.0510 - val_categorical_accuracy: 0.9937\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.05144 to 0.05099, saving model to weights_base_20000_150_0.2.hdf5\n",
      "Epoch 3/4\n",
      "114890/114890 [==============================] - 1193s 10ms/step - loss: 0.0396 - categorical_accuracy: 0.9786 - val_loss: 0.0549 - val_categorical_accuracy: 0.9779\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/4\n",
      "114890/114890 [==============================] - 1186s 10ms/step - loss: 0.0343 - categorical_accuracy: 0.9681 - val_loss: 0.0557 - val_categorical_accuracy: 0.9915\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "padding maxlen= 250\n",
      "building model\n",
      "fitting model with max_features=20000, maxlen = 250, and dropout = 0.1\n",
      "Train on 114890 samples, validate on 12766 samples\n",
      "Epoch 1/4\n",
      "114890/114890 [==============================] - 1839s 16ms/step - loss: 0.0659 - categorical_accuracy: 0.9414 - val_loss: 0.0506 - val_categorical_accuracy: 0.9939\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.05057, saving model to weights_base_20000_250_0.1.hdf5\n",
      "Epoch 2/4\n",
      "114890/114890 [==============================] - 1840s 16ms/step - loss: 0.0455 - categorical_accuracy: 0.9759 - val_loss: 0.0498 - val_categorical_accuracy: 0.9908\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.05057 to 0.04985, saving model to weights_base_20000_250_0.1.hdf5\n",
      "Epoch 3/4\n",
      "114890/114890 [==============================] - 1849s 16ms/step - loss: 0.0395 - categorical_accuracy: 0.9528 - val_loss: 0.0485 - val_categorical_accuracy: 0.9776\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.04985 to 0.04849, saving model to weights_base_20000_250_0.1.hdf5\n",
      "Epoch 4/4\n",
      "114890/114890 [==============================] - 1850s 16ms/step - loss: 0.0339 - categorical_accuracy: 0.9352 - val_loss: 0.0509 - val_categorical_accuracy: 0.9781\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "padding maxlen= 250\n",
      "building model\n",
      "fitting model with max_features=20000, maxlen = 250, and dropout = 0.2\n",
      "Train on 114890 samples, validate on 12766 samples\n",
      "Epoch 1/4\n",
      "114890/114890 [==============================] - 1836s 16ms/step - loss: 0.0676 - categorical_accuracy: 0.9387 - val_loss: 0.0525 - val_categorical_accuracy: 0.9939\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.05253, saving model to weights_base_20000_250_0.2.hdf5\n",
      "Epoch 2/4\n",
      "114890/114890 [==============================] - 1851s 16ms/step - loss: 0.0458 - categorical_accuracy: 0.9844 - val_loss: 0.0500 - val_categorical_accuracy: 0.9919\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.05253 to 0.04999, saving model to weights_base_20000_250_0.2.hdf5\n",
      "Epoch 3/4\n",
      "114890/114890 [==============================] - 1901s 17ms/step - loss: 0.0405 - categorical_accuracy: 0.9808 - val_loss: 0.0497 - val_categorical_accuracy: 0.9930\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.04999 to 0.04969, saving model to weights_base_20000_250_0.2.hdf5\n",
      "Epoch 4/4\n",
      "114890/114890 [==============================] - 1901s 17ms/step - loss: 0.0351 - categorical_accuracy: 0.9448 - val_loss: 0.0515 - val_categorical_accuracy: 0.9911\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n"
     ]
    }
   ],
   "source": [
    "max_feature_list=[20000,10000]\n",
    "maxlen_list=[150,250] \n",
    "dropout_list=[0.1,0.2]\n",
    "EPOCHS=4\n",
    "\n",
    "for max_feature in [max_feature_list[0]]:\n",
    "    \n",
    "    for maxlen in maxlen_list:\n",
    "        \n",
    "        for dropout in dropout_list:\n",
    "            \n",
    "            print('padding maxlen=',maxlen)\n",
    "            x_train_pad = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "            x_test_pad = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "            \n",
    "            print('building model')\n",
    "            model=get_model(maxlen=maxlen,max_features=max_feature,dropout=dropout,embed_size=128)\n",
    "            \n",
    "            batch_size = 32\n",
    "\n",
    "\n",
    "            weight_file_path=\"weights_base_{}_{}_{}.hdf5\".format(max_feature,maxlen,dropout)\n",
    "            model_file_path='bidirectional_lstm_globMP_relu_sigmoid_maxfeat{}_maxlen{}_dropout{}'.format(max_feature,maxlen,dropout)\n",
    "\n",
    "            checkpoint = ModelCheckpoint(weight_file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "            early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=20)\n",
    "\n",
    "            callbacks_list = [checkpoint, early] #early\n",
    "            \n",
    "            #fit model\n",
    "            print('fitting model with max_features={}, maxlen = {}, and dropout = {}'.format(max_feature,maxlen,dropout))\n",
    "            model.fit(x_train_pad, y_train, batch_size=batch_size, epochs=EPOCHS, validation_split=0.1, callbacks=callbacks_list)\n",
    "            \n",
    "            #save model\n",
    "            model.save(model_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'embedding_4/embeddings:0' shape=(20000, 128) dtype=float32_ref>,\n",
       " <tf.Variable 'bidirectional_4/forward_lstm_4/kernel:0' shape=(128, 200) dtype=float32_ref>,\n",
       " <tf.Variable 'bidirectional_4/forward_lstm_4/recurrent_kernel:0' shape=(50, 200) dtype=float32_ref>,\n",
       " <tf.Variable 'bidirectional_4/forward_lstm_4/bias:0' shape=(200,) dtype=float32_ref>,\n",
       " <tf.Variable 'bidirectional_4/backward_lstm_4/kernel:0' shape=(128, 200) dtype=float32_ref>,\n",
       " <tf.Variable 'bidirectional_4/backward_lstm_4/recurrent_kernel:0' shape=(50, 200) dtype=float32_ref>,\n",
       " <tf.Variable 'bidirectional_4/backward_lstm_4/bias:0' shape=(200,) dtype=float32_ref>,\n",
       " <tf.Variable 'dense_7/kernel:0' shape=(100, 50) dtype=float32_ref>,\n",
       " <tf.Variable 'dense_7/bias:0' shape=(50,) dtype=float32_ref>,\n",
       " <tf.Variable 'dense_8/kernel:0' shape=(50, 6) dtype=float32_ref>,\n",
       " <tf.Variable 'dense_8/bias:0' shape=(6,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model=get_model(maxlen=100,max_features=20000,dropout=0.1,embed_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model \n",
    "\n",
    "# Returns a compiled model identical to the previous one\n",
    "#model = load_model(\"model_name.hdf5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = model.predict(x_test_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31915, 6)\n",
      "(31915, 6)\n"
     ]
    }
   ],
   "source": [
    "print(y_test.shape)\n",
    "print(y_predict.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model=model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31915/31915 [==============================] - 109s 3ms/step\n",
      "categorical_accuracy: 99.24%\n"
     ]
    }
   ],
   "source": [
    "#binary_accuracy(y_true, y_pred)\n",
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss='binary_crossentropy',optimizer='adam',metrics=[categorical_accuracy])\n",
    "score = loaded_model.evaluate(x_test_pad, y_test, verbose=1)\n",
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
