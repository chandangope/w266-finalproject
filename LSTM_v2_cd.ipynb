{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "143613/143613 [==============================] - 1069s 7ms/step - loss: 0.2844 - acc: 0.9907 - val_loss: 0.2791 - val_acc: 0.9939\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.27913, saving model to weights_base.best.hdf5\n",
      "Epoch 2/2\n",
      "143613/143613 [==============================] - 1116s 8ms/step - loss: 0.2668 - acc: 0.9926 - val_loss: 0.2764 - val_acc: 0.9896\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.27913 to 0.27637, saving model to weights_base.best.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x128f59c50>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Embedding, Input\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "max_features = 20000\n",
    "maxlen = 100\n",
    "\n",
    "\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "#test = pd.read_csv(\"test.csv\")\n",
    "#train = train.sample(frac=1)\n",
    "\n",
    "list_sentences_train = train[\"comment_text\"].fillna(\"nada\").values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "#list_sentences_test = test[\"comment_text\"].fillna(\"nada\").values\n",
    "\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "#list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_t = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "#X_te = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "\n",
    "def get_model():\n",
    "    embed_size = 128\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size)(inp)\n",
    "    x = Bidirectional(LSTM(50, return_sequences=True))(x)\n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(50, activation=\"relu\")(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(6, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model = get_model()\n",
    "batch_size = 32\n",
    "epochs = 2\n",
    "\n",
    "\n",
    "file_path=\"weights_base.best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=20)\n",
    "\n",
    "\n",
    "callbacks_list = [checkpoint, early] #early\n",
    "model.fit(X_t, y, batch_size=batch_size, epochs=epochs, validation_split=0.1, callbacks=callbacks_list)\n",
    "\n",
    "#model.load_weights(file_path)\n",
    "\n",
    "#y_predict = model.predict(X_te)\n",
    "\n",
    "\n",
    "\n",
    "#sample_submission = pd.read_csv(\"sample_submission.csv\")\n",
    "\n",
    "#sample_submission[list_classes] = y_predict\n",
    "\n",
    "\n",
    "\n",
    "#sample_submission.to_csv(\"baseline.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#maxlens=[100,200,300]\n",
    "#batch_sizes=[32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_len_bin=dict()\n",
    "\n",
    "comment_len_bin['total']=train.shape[0]\n",
    "comment_len_bin['100']=0\n",
    "comment_len_bin['150']=0\n",
    "comment_len_bin['200']=0\n",
    "comment_len_bin['250']=0\n",
    "comment_len_bin['300']=0\n",
    "\n",
    "for index,row in train.iterrows():\n",
    "    \n",
    "    if len(row['comment_text'].split(' '))<=100:\n",
    "        comment_len_bin['100']+=(1)\n",
    "        \n",
    "    if len(row['comment_text'].split(' '))<=150:\n",
    "        comment_len_bin['150']+=(1)\n",
    "\n",
    "    if len(row['comment_text'].split(' '))<=200:\n",
    "        comment_len_bin['200']+=(1)\n",
    "\n",
    "    if len(row['comment_text'].split(' '))<=250:\n",
    "        comment_len_bin['250']+=(1)\n",
    "\n",
    "    if len(row['comment_text'].split(' '))<=300:\n",
    "        comment_len_bin['300']+=(1)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'100': 130994, '150': 0, '200': 0, '250': 0, '300': 0, 'total': 159571}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment_len_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 82.09 % of comments have length equal or less than 100 words\n",
      " 89.78 % of comments have length equal or less than 150 words\n",
      " 93.61 % of comments have length equal or less than 200 words\n",
      " 95.75 % of comments have length equal or less than 250 words\n",
      " 96.95 % of comments have length equal or less than 300 words\n"
     ]
    }
   ],
   "source": [
    "for i in ['100','150','200','250','300']:\n",
    "    print('{:6.2f}'.format(100*comment_len_bin[i]/comment_len_bin['total']),'% of comments have length equal or less than', i, 'words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets Automate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget('https://s3.amazonaws.com/danicic-w266-final/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from scipy.sparse import hstack\n",
    "from sklearn import model_selection\n",
    "import numpy as np\n",
    "import tensorflow  as tf\n",
    "import keras\n",
    "import h5py\n",
    "from keras.models import load_model\n",
    "\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Embedding, Input\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from keras.metrics import categorical_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing data...\n",
      "Splitting data...\n",
      "127656 train sequences\n",
      "31915 test sequences\n"
     ]
    }
   ],
   "source": [
    "# import data\n",
    "print('Importing data...')\n",
    "train_all=pd.read_csv('train.csv')\n",
    "\n",
    "# 80/20 split\n",
    "train_80, test_20 = model_selection.train_test_split(train_all,test_size=0.2)\n",
    "\n",
    "#preprocessing\n",
    "\n",
    "print('Splitting data...')\n",
    "\n",
    "#training data\n",
    "x_train = train_80['comment_text'].fillna(\"nada\").values\n",
    "y_train = train_80[['toxic','severe_toxic','obscene','threat','insult','identity_hate']].values\n",
    "\n",
    "#testing validation data (not for training model, just validation for )\n",
    "\n",
    "x_test = test_20['comment_text'].fillna(\"nada\").values\n",
    "y_test = test_20[['toxic','severe_toxic','obscene','threat','insult','identity_hate']].values\n",
    "\n",
    "# check lengths\n",
    "\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_feature_list=[20000,10000]\n",
    "EPOCHS=4\n",
    "maxlen_list=[150,250] \n",
    "dropout_list=[0.1,0.2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = text.Tokenizer(num_words=max_feature_list[0])\n",
    "\n",
    "tokenizer.fit_on_texts(list(train_all[\"comment_text\"].fillna(\"nada\").values)) #fit on all comment_text\n",
    "\n",
    "#create tokenized comments\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(x_train)  #training (80% of train.csv)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(x_test)  #testing (20% of train.csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create LSTM model maker function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(maxlen,max_features,dropout,embed_size=128):\n",
    "    \n",
    "    embed_size=embed_size #default to 128\n",
    "    maxlen=maxlen # max length of sequence input\n",
    "    max_features=max_features  # max vocab\n",
    "    dropout=dropout\n",
    "    \n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    \n",
    "    x = Embedding(max_features, embed_size)(inp)\n",
    "    \n",
    "    x = Bidirectional(LSTM(50, return_sequences=True))(x)\n",
    "    \n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    \n",
    "    x = Dropout(dropout)(x)\n",
    "    \n",
    "    x = Dense(50, activation=\"relu\")(x)\n",
    "    \n",
    "    x = Dropout(dropout)(x)\n",
    "    \n",
    "    x = Dense(6, activation=\"sigmoid\")(x)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy',optimizer='adam',metrics=[categorical_accuracy])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize and fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "padding maxlen= 150\n",
      "building model\n",
      "fitting model with max_features=20000, maxlen = 150, and dropout = 0.1\n",
      "Train on 114890 samples, validate on 12766 samples\n",
      "Epoch 1/4\n",
      "114890/114890 [==============================] - 1211s 11ms/step - loss: 0.0658 - categorical_accuracy: 0.9377 - val_loss: 0.0499 - val_categorical_accuracy: 0.9939\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04986, saving model to weights_base_20000_150_0.1.hdf5\n",
      "Epoch 2/4\n",
      "114890/114890 [==============================] - 1234s 11ms/step - loss: 0.0444 - categorical_accuracy: 0.9656 - val_loss: 0.0482 - val_categorical_accuracy: 0.9925\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.04986 to 0.04824, saving model to weights_base_20000_150_0.1.hdf5\n",
      "Epoch 3/4\n",
      "114890/114890 [==============================] - 1198s 10ms/step - loss: 0.0380 - categorical_accuracy: 0.9620 - val_loss: 0.0499 - val_categorical_accuracy: 0.9926\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/4\n",
      "114890/114890 [==============================] - 1187s 10ms/step - loss: 0.0328 - categorical_accuracy: 0.9030 - val_loss: 0.0558 - val_categorical_accuracy: 0.9907\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "padding maxlen= 150\n",
      "building model\n",
      "fitting model with max_features=20000, maxlen = 150, and dropout = 0.2\n",
      "Train on 114890 samples, validate on 12766 samples\n",
      "Epoch 1/4\n",
      "114890/114890 [==============================] - 1198s 10ms/step - loss: 0.0658 - categorical_accuracy: 0.9285 - val_loss: 0.0514 - val_categorical_accuracy: 0.9939\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.05144, saving model to weights_base_20000_150_0.2.hdf5\n",
      "Epoch 2/4\n",
      "114890/114890 [==============================] - 1198s 10ms/step - loss: 0.0457 - categorical_accuracy: 0.9861 - val_loss: 0.0510 - val_categorical_accuracy: 0.9937\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.05144 to 0.05099, saving model to weights_base_20000_150_0.2.hdf5\n",
      "Epoch 3/4\n",
      "114890/114890 [==============================] - 1193s 10ms/step - loss: 0.0396 - categorical_accuracy: 0.9786 - val_loss: 0.0549 - val_categorical_accuracy: 0.9779\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/4\n",
      "114890/114890 [==============================] - 1186s 10ms/step - loss: 0.0343 - categorical_accuracy: 0.9681 - val_loss: 0.0557 - val_categorical_accuracy: 0.9915\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "padding maxlen= 250\n",
      "building model\n",
      "fitting model with max_features=20000, maxlen = 250, and dropout = 0.1\n",
      "Train on 114890 samples, validate on 12766 samples\n",
      "Epoch 1/4\n",
      "114890/114890 [==============================] - 1839s 16ms/step - loss: 0.0659 - categorical_accuracy: 0.9414 - val_loss: 0.0506 - val_categorical_accuracy: 0.9939\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.05057, saving model to weights_base_20000_250_0.1.hdf5\n",
      "Epoch 2/4\n",
      "114890/114890 [==============================] - 1840s 16ms/step - loss: 0.0455 - categorical_accuracy: 0.9759 - val_loss: 0.0498 - val_categorical_accuracy: 0.9908\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.05057 to 0.04985, saving model to weights_base_20000_250_0.1.hdf5\n",
      "Epoch 3/4\n",
      "114890/114890 [==============================] - 1849s 16ms/step - loss: 0.0395 - categorical_accuracy: 0.9528 - val_loss: 0.0485 - val_categorical_accuracy: 0.9776\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.04985 to 0.04849, saving model to weights_base_20000_250_0.1.hdf5\n",
      "Epoch 4/4\n",
      "114890/114890 [==============================] - 1850s 16ms/step - loss: 0.0339 - categorical_accuracy: 0.9352 - val_loss: 0.0509 - val_categorical_accuracy: 0.9781\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "padding maxlen= 250\n",
      "building model\n",
      "fitting model with max_features=20000, maxlen = 250, and dropout = 0.2\n",
      "Train on 114890 samples, validate on 12766 samples\n",
      "Epoch 1/4\n",
      "114890/114890 [==============================] - 1836s 16ms/step - loss: 0.0676 - categorical_accuracy: 0.9387 - val_loss: 0.0525 - val_categorical_accuracy: 0.9939\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.05253, saving model to weights_base_20000_250_0.2.hdf5\n",
      "Epoch 2/4\n",
      "114890/114890 [==============================] - 1851s 16ms/step - loss: 0.0458 - categorical_accuracy: 0.9844 - val_loss: 0.0500 - val_categorical_accuracy: 0.9919\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.05253 to 0.04999, saving model to weights_base_20000_250_0.2.hdf5\n",
      "Epoch 3/4\n",
      "114890/114890 [==============================] - 1901s 17ms/step - loss: 0.0405 - categorical_accuracy: 0.9808 - val_loss: 0.0497 - val_categorical_accuracy: 0.9930\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.04999 to 0.04969, saving model to weights_base_20000_250_0.2.hdf5\n",
      "Epoch 4/4\n",
      "114890/114890 [==============================] - 1901s 17ms/step - loss: 0.0351 - categorical_accuracy: 0.9448 - val_loss: 0.0515 - val_categorical_accuracy: 0.9911\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n"
     ]
    }
   ],
   "source": [
    "max_feature_list=[20000,10000]\n",
    "maxlen_list=[150,250] \n",
    "dropout_list=[0.1,0.2]\n",
    "EPOCHS=4\n",
    "\n",
    "for max_feature in [max_feature_list[0]]:\n",
    "    \n",
    "    for maxlen in maxlen_list:\n",
    "        \n",
    "        for dropout in dropout_list:\n",
    "            \n",
    "            print('padding maxlen=',maxlen)\n",
    "            x_train_pad = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "            x_test_pad = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "            \n",
    "            print('building model')\n",
    "            model=get_model(maxlen=maxlen,max_features=max_feature,dropout=dropout,embed_size=128)\n",
    "            \n",
    "            batch_size = 32\n",
    "\n",
    "\n",
    "            weight_file_path=\"weights_base_{}_{}_{}.hdf5\".format(max_feature,maxlen,dropout)\n",
    "            model_file_path='bidirectional_lstm_globMP_relu_sigmoid_maxfeat{}_maxlen{}_dropout{}'.format(max_feature,maxlen,dropout)\n",
    "\n",
    "            checkpoint = ModelCheckpoint(weight_file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "            early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=20)\n",
    "\n",
    "            callbacks_list = [checkpoint, early] #early\n",
    "            \n",
    "            #fit model\n",
    "            print('fitting model with max_features={}, maxlen = {}, and dropout = {}'.format(max_feature,maxlen,dropout))\n",
    "            model.fit(x_train_pad, y_train, batch_size=batch_size, epochs=EPOCHS, validation_split=0.1, callbacks=callbacks_list)\n",
    "            \n",
    "            #save model\n",
    "            model.save(model_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'embedding_4/embeddings:0' shape=(20000, 128) dtype=float32_ref>,\n",
       " <tf.Variable 'bidirectional_4/forward_lstm_4/kernel:0' shape=(128, 200) dtype=float32_ref>,\n",
       " <tf.Variable 'bidirectional_4/forward_lstm_4/recurrent_kernel:0' shape=(50, 200) dtype=float32_ref>,\n",
       " <tf.Variable 'bidirectional_4/forward_lstm_4/bias:0' shape=(200,) dtype=float32_ref>,\n",
       " <tf.Variable 'bidirectional_4/backward_lstm_4/kernel:0' shape=(128, 200) dtype=float32_ref>,\n",
       " <tf.Variable 'bidirectional_4/backward_lstm_4/recurrent_kernel:0' shape=(50, 200) dtype=float32_ref>,\n",
       " <tf.Variable 'bidirectional_4/backward_lstm_4/bias:0' shape=(200,) dtype=float32_ref>,\n",
       " <tf.Variable 'dense_7/kernel:0' shape=(100, 50) dtype=float32_ref>,\n",
       " <tf.Variable 'dense_7/bias:0' shape=(50,) dtype=float32_ref>,\n",
       " <tf.Variable 'dense_8/kernel:0' shape=(50, 6) dtype=float32_ref>,\n",
       " <tf.Variable 'dense_8/bias:0' shape=(6,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model=get_model(maxlen=100,max_features=20000,dropout=0.1,embed_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model \n",
    "\n",
    "# Returns a compiled model identical to the previous one\n",
    "#model = load_model(\"model_name.hdf5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = model.predict(x_test_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31915, 6)\n",
      "(31915, 6)\n"
     ]
    }
   ],
   "source": [
    "print(y_test.shape)\n",
    "print(y_predict.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model=model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31915/31915 [==============================] - 109s 3ms/step\n",
      "categorical_accuracy: 99.24%\n"
     ]
    }
   ],
   "source": [
    "#binary_accuracy(y_true, y_pred)\n",
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss='binary_crossentropy',optimizer='adam',metrics=[categorical_accuracy])\n",
    "score = loaded_model.evaluate(x_test_pad, y_test, verbose=1)\n",
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict={'bidirectional_lstm_globMP_relu_sigmoid_maxfeat20000_maxlen150_dropout0.1':[150,0.1],\\\n",
    "'bidirectional_lstm_globMP_relu_sigmoid_maxfeat20000_maxlen150_dropout0.2':[150,0.2],\\\n",
    "'bidirectional_lstm_globMP_relu_sigmoid_maxfeat20000_maxlen250_dropout0.1':[250,0.1],\\\n",
    "'bidirectional_lstm_globMP_relu_sigmoid_maxfeat20000_maxlen250_dropout0.2':[250,0.2]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------- \n",
      "Model with maxlen of 150 and dropout of 0.1\n",
      "padding maxlen= 150\n",
      "loading model\n",
      "compiling model\n",
      "evaluating model\n",
      "31915/31915 [==============================] - 67s 2ms/step\n",
      "categorical_accuracy: 99.11%\n",
      "-------------------------------------------------- \n",
      "\n",
      "-------------------------------------------------- \n",
      "Model with maxlen of 150 and dropout of 0.2\n",
      "padding maxlen= 150\n",
      "loading model\n",
      "compiling model\n",
      "evaluating model\n",
      "31915/31915 [==============================] - 84s 3ms/step\n",
      "categorical_accuracy: 99.26%\n",
      "-------------------------------------------------- \n",
      "\n",
      "-------------------------------------------------- \n",
      "Model with maxlen of 250 and dropout of 0.1\n",
      "padding maxlen= 250\n",
      "loading model\n",
      "compiling model\n",
      "evaluating model\n",
      "31915/31915 [==============================] - 127s 4ms/step\n",
      "categorical_accuracy: 97.69%\n",
      "-------------------------------------------------- \n",
      "\n",
      "-------------------------------------------------- \n",
      "Model with maxlen of 250 and dropout of 0.2\n",
      "padding maxlen= 250\n",
      "loading model\n",
      "compiling model\n",
      "evaluating model\n",
      "31915/31915 [==============================] - 120s 4ms/step\n",
      "categorical_accuracy: 99.24%\n",
      "-------------------------------------------------- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model \n",
    "\n",
    "# Returns a compiled model identical to the previous one\n",
    "#model = load_model(\"model_name.hdf5\")\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=20000)\n",
    "#tokenize based on comment database\n",
    "tokenizer.fit_on_texts(list(train_all[\"comment_text\"].fillna(\"nada\").values)) #fit on all comment_text\n",
    "#create tokenized comments\n",
    "#list_tokenized_train = tokenizer.texts_to_sequences(x_train)  #training (80% of train.csv)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(x_test)  #testing (20% of train.csv)\n",
    "\n",
    "for file in model_dict.keys():\n",
    "    \n",
    "    maxlen=model_dict[file][0]\n",
    "    dropout=model_dict[file][1]\n",
    "    \n",
    "    print('-'*50,'\\nModel with maxlen of {} and dropout of {}'.format(maxlen,dropout))\n",
    "    \n",
    "    print('padding maxlen=',maxlen)\n",
    "    #x_train_pad = sequence.pad_sequences(list_tokenized_train, maxlen=model_dict[file])\n",
    "    x_test_pad = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "\n",
    "    print('loading model')\n",
    "    loaded_model=load_model(file)\n",
    "    \n",
    "    print('compiling model')\n",
    "    loaded_model.compile(loss='binary_crossentropy',optimizer='adam',metrics=[categorical_accuracy])\n",
    "    print('evaluating model')\n",
    "    score = loaded_model.evaluate(x_test_pad, y_test, verbose=1)\n",
    "    print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))\n",
    "    print('-'*50,'\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing data...\n",
      "Splitting data...\n",
      "127656 train sequences\n",
      "31915 test sequences\n",
      "padding maxlen= 150\n",
      "building model\n",
      "fitting model with max_features=10000, maxlen = 150, and dropout = 0.1\n",
      "Train on 114890 samples, validate on 12766 samples\n",
      "Epoch 1/4\n",
      "114890/114890 [==============================] - 1488s 13ms/step - loss: 0.0701 - categorical_accuracy: 0.9329 - val_loss: 0.0488 - val_categorical_accuracy: 0.9939\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04875, saving model to weights_base_10000_150_0.1.hdf5\n",
      "Epoch 2/4\n",
      "114890/114890 [==============================] - 1410s 12ms/step - loss: 0.0482 - categorical_accuracy: 0.9766 - val_loss: 0.0470 - val_categorical_accuracy: 0.9934\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.04875 to 0.04698, saving model to weights_base_10000_150_0.1.hdf5\n",
      "Epoch 3/4\n",
      "114890/114890 [==============================] - 1154s 10ms/step - loss: 0.0426 - categorical_accuracy: 0.9715 - val_loss: 0.0461 - val_categorical_accuracy: 0.9934\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.04698 to 0.04615, saving model to weights_base_10000_150_0.1.hdf5\n",
      "Epoch 4/4\n",
      "114890/114890 [==============================] - 1153s 10ms/step - loss: 0.0374 - categorical_accuracy: 0.9716 - val_loss: 0.0528 - val_categorical_accuracy: 0.9929\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n"
     ]
    }
   ],
   "source": [
    "# import data\n",
    "print('Importing data...')\n",
    "train_all=pd.read_csv('train.csv')\n",
    "\n",
    "# 80/20 split\n",
    "train_80, test_20 = model_selection.train_test_split(train_all,test_size=0.2)\n",
    "\n",
    "#preprocessing\n",
    "\n",
    "print('Splitting data...')\n",
    "\n",
    "#training data\n",
    "x_train = train_80['comment_text'].fillna(\"nada\").values\n",
    "y_train = train_80[['toxic','severe_toxic','obscene','threat','insult','identity_hate']].values\n",
    "\n",
    "#testing validation data (not for training model, just validation for )\n",
    "\n",
    "x_test = test_20['comment_text'].fillna(\"nada\").values\n",
    "y_test = test_20[['toxic','severe_toxic','obscene','threat','insult','identity_hate']].values\n",
    "\n",
    "# check lengths\n",
    "\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "\n",
    "\n",
    "# MODEL PARAMETERS\n",
    "\n",
    "max_feature_list=[20000,10000]\n",
    "EPOCHS=4\n",
    "maxlen_list=[150,250] \n",
    "dropout_list=[0.1,0.2]\n",
    "\n",
    "#TOKENIZE \n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=max_feature_list[1])\n",
    "tokenizer.fit_on_texts(list(train_all[\"comment_text\"].fillna(\"nada\").values)) #fit on all comment_text\n",
    "\n",
    "#create tokenized comments\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(x_train)  #training (80% of train.csv)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(x_test)  #testing (20% of train.csv)\n",
    "\n",
    "\n",
    "\n",
    "for max_feature in [max_feature_list[1]]:\n",
    "    \n",
    "    for maxlen in [maxlen_list[0]]:\n",
    "        \n",
    "        for dropout in [dropout_list[0]]:\n",
    "            \n",
    "            print('padding maxlen=',maxlen)\n",
    "            x_train_pad = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "            x_test_pad = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "            \n",
    "            print('building model')\n",
    "            model=get_model(maxlen=maxlen,max_features=max_feature,dropout=dropout,embed_size=128)\n",
    "            \n",
    "            batch_size = 32\n",
    "\n",
    "\n",
    "            weight_file_path=\"weights_base_{}_{}_{}.hdf5\".format(max_feature,maxlen,dropout)\n",
    "            model_file_path='bidirectional_lstm_globMP_relu_sigmoid_maxfeat{}_maxlen{}_dropout{}'.format(max_feature,maxlen,dropout)\n",
    "\n",
    "            checkpoint = ModelCheckpoint(weight_file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "            early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=20)\n",
    "\n",
    "            callbacks_list = [checkpoint, early] #early\n",
    "            \n",
    "            #fit model\n",
    "            print('fitting model with max_features={}, maxlen = {}, and dropout = {}'.format(max_feature,maxlen,dropout))\n",
    "            model.fit(x_train_pad, y_train, batch_size=batch_size, epochs=EPOCHS, validation_split=0.1, callbacks=callbacks_list)\n",
    "            \n",
    "            #save model\n",
    "            model.save(model_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compiling model\n",
      "evaluating model\n",
      "31915/31915 [==============================] - 78s 2ms/step\n",
      "categorical_accuracy: 99.32%\n",
      "-------------------------------------------------- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "loaded_model=model\n",
    "\n",
    "x_test_pad = sequence.pad_sequences(list_tokenized_test, maxlen=150)\n",
    "\n",
    "print('compiling model')\n",
    "loaded_model.compile(loss='binary_crossentropy',optimizer='adam',metrics=[categorical_accuracy])\n",
    "print('evaluating model')\n",
    "score = loaded_model.evaluate(x_test_pad, y_test, verbose=1)\n",
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))\n",
    "print('-'*50,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict=model.predict(x_test_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict=pd.DataFrame(y_predict)\n",
    "y_test=pd.DataFrame(y_test)\n",
    "\n",
    "y_predict.columns=['toxic','severe_toxic','obscene','threat','insult','identity_hate']\n",
    "y_test.columns=['toxic','severe_toxic','obscene','threat','insult','identity_hate']\n",
    "\n",
    "\n",
    "# get predictions with decision boundary = 0.5\n",
    "for c in [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]:\n",
    "\n",
    "    y_predict[c]=y_predict[c].map(lambda x: 1 if x >=0.5 else 0)\n",
    "    \n",
    "# subract predictions from true labels to get type I (= -1) and type II (=1)\n",
    "y_error=y_test-y_predict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    toxic  severe_toxic  obscene  threat  insult  identity_hate\n",
       "0       0             0        0       0       0              0\n",
       "1       0             0        0       0       0              0\n",
       "2       0             0        0       0       0              0\n",
       "3       0             0        0       0       0              0\n",
       "4       0             0        0       0       0              0\n",
       "5       0             0        0       0       0              0\n",
       "6       0             0        0       0       0              0\n",
       "7       0             0        0       0       0              0\n",
       "8       0             0        0       0       0              0\n",
       "9       0             0        0       0       0              0\n",
       "10      0             0        0       0       0              0\n",
       "11      0             0        0       0       0              0\n",
       "12      0             0        0       0       0              0\n",
       "13      0             0        0       0       0              0\n",
       "14      0             0        0       0       0              0\n",
       "15      0             0        0       0       0              0\n",
       "16      0             0        0       0       0              0\n",
       "17      0             0        0       0       0              0\n",
       "18      1             0        0       0       0              0\n",
       "19      0             0        0       0       0              0"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_error.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"\\n\\nMay Allah (swt) either give you punishment or hidiyaat for spreading falsehood and keeping other Muslim editors from making the article neutral - Insh\\'Allah.  —Preceding unsigned comment added by 89.108.24.87     —Preceding unsigned comment added by 213.146.172.146   \\n\\nYour comment on your edit proves you are ignorant of Islam.  —Preceding unsigned comment added by 213.146.172.146   \"'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_20['comment_text'].iloc[18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic\n",
      " 0    30690\n",
      " 1      992\n",
      "-1      233\n",
      "Name: toxic, dtype: int64\n",
      "\n",
      " Type I (flagged when no flag exists): 0.73%\n",
      "Type II (not flagged when flag should exist): 3.11%\n",
      "\n",
      " \n",
      "\n",
      "severe_toxic\n",
      " 0    31611\n",
      " 1      187\n",
      "-1      117\n",
      "Name: severe_toxic, dtype: int64\n",
      "\n",
      " Type I (flagged when no flag exists): 0.37%\n",
      "Type II (not flagged when flag should exist): 0.59%\n",
      "\n",
      " \n",
      "\n",
      "obscene\n",
      " 0    31292\n",
      " 1      384\n",
      "-1      239\n",
      "Name: obscene, dtype: int64\n",
      "\n",
      " Type I (flagged when no flag exists): 0.75%\n",
      "Type II (not flagged when flag should exist): 1.20%\n",
      "\n",
      " \n",
      "\n",
      "threat\n",
      "0    31829\n",
      "1       86\n",
      "Name: threat, dtype: int64\n",
      "\n",
      " Type I (flagged when no flag exists): 0.00%\n",
      "Type II (not flagged when flag should exist): 0.27%\n",
      "\n",
      " \n",
      "\n",
      "insult\n",
      " 0    31060\n",
      " 1      532\n",
      "-1      323\n",
      "Name: insult, dtype: int64\n",
      "\n",
      " Type I (flagged when no flag exists): 1.01%\n",
      "Type II (not flagged when flag should exist): 1.67%\n",
      "\n",
      " \n",
      "\n",
      "identity_hate\n",
      " 0    31675\n",
      " 1      161\n",
      "-1       79\n",
      "Name: identity_hate, dtype: int64\n",
      "\n",
      " Type I (flagged when no flag exists): 0.25%\n",
      "Type II (not flagged when flag should exist): 0.50%\n",
      "\n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "tot=y_error.shape[0]\n",
    "for c in [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]:\n",
    "    print(c)\n",
    "    print(y_error[c].value_counts())\n",
    "    type1=0\n",
    "    type2=0\n",
    "    \n",
    "    try:\n",
    "        type1=y_error[c].value_counts()[-1]\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        type2=y_error[c].value_counts()[1]\n",
    "    except:\n",
    "        pass\n",
    "    print('\\n Type I (flagged when no flag exists):','{:02.2f}%'.format(type1*100/tot))\n",
    "    print('Type II (not flagged when flag should exist):',\"{:02.2f}%\".format(type2*100/tot))\n",
    "    print('\\n \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31915, 8)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_20.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'dense_11/kernel:0' shape=(100, 50) dtype=float32_ref>\n"
     ]
    }
   ],
   "source": [
    "model.weights[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07773774087419709\n"
     ]
    }
   ],
   "source": [
    "\n",
    "wrong=0\n",
    "for index, row in y_error.iterrows():\n",
    "    if y_error.iloc[index].value_counts()[0]!=6:\n",
    "        wrong+=1\n",
    "        \n",
    "print(wrong/y_error.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_error.iloc[1].value_counts()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3333"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([992,233,187,117,384,239,86,532,323,161,79])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.012956290145699515"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.07773774087419709/6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOW LETS AUTOMATE MAX FEATURES 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_feature_list=[20000,10000]\n",
    "EPOCHS=4\n",
    "maxlen_list=[150,250] \n",
    "dropout_list=[0.1,0.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=max_feature_list[1])\n",
    "\n",
    "tokenizer.fit_on_texts(list(train_all[\"comment_text\"].fillna(\"nada\").values)) #fit on all comment_text\n",
    "\n",
    "#create tokenized comments\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(x_train)  #training (80% of train.csv)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(x_test)  #testing (20% of train.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "padding maxlen= 150\n",
      "building model\n",
      "fitting model with max_features=10000, maxlen = 150, and dropout = 0.2\n",
      "Train on 114890 samples, validate on 12766 samples\n",
      "Epoch 1/4\n",
      "114890/114890 [==============================] - 1208s 11ms/step - loss: 0.0695 - categorical_accuracy: 0.9150 - val_loss: 0.0489 - val_categorical_accuracy: 0.9940\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04887, saving model to weights_base_10000_150_0.2.hdf5\n",
      "Epoch 2/4\n",
      "114890/114890 [==============================] - 1168s 10ms/step - loss: 0.0488 - categorical_accuracy: 0.9832 - val_loss: 0.0475 - val_categorical_accuracy: 0.9940\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.04887 to 0.04755, saving model to weights_base_10000_150_0.2.hdf5\n",
      "Epoch 3/4\n",
      "114890/114890 [==============================] - 1158s 10ms/step - loss: 0.0437 - categorical_accuracy: 0.9822 - val_loss: 0.0469 - val_categorical_accuracy: 0.9938\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.04755 to 0.04693, saving model to weights_base_10000_150_0.2.hdf5\n",
      "Epoch 4/4\n",
      "114890/114890 [==============================] - 1153s 10ms/step - loss: 0.0392 - categorical_accuracy: 0.9637 - val_loss: 0.0507 - val_categorical_accuracy: 0.9940\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "padding maxlen= 250\n",
      "building model\n",
      "fitting model with max_features=10000, maxlen = 250, and dropout = 0.1\n",
      "Train on 114890 samples, validate on 12766 samples\n",
      "Epoch 1/4\n",
      "114890/114890 [==============================] - 1877s 16ms/step - loss: 0.0671 - categorical_accuracy: 0.9216 - val_loss: 0.0493 - val_categorical_accuracy: 0.9934\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04929, saving model to weights_base_10000_250_0.1.hdf5\n",
      "Epoch 2/4\n",
      "114890/114890 [==============================] - 1819s 16ms/step - loss: 0.0476 - categorical_accuracy: 0.9732 - val_loss: 0.0471 - val_categorical_accuracy: 0.9925\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.04929 to 0.04713, saving model to weights_base_10000_250_0.1.hdf5\n",
      "Epoch 3/4\n",
      "114890/114890 [==============================] - 1807s 16ms/step - loss: 0.0423 - categorical_accuracy: 0.9692 - val_loss: 0.0484 - val_categorical_accuracy: 0.9940\n",
      "\n",
      "Epoch 00003: val_loss did not improve\n",
      "Epoch 4/4\n",
      "114890/114890 [==============================] - 1790s 16ms/step - loss: 0.0373 - categorical_accuracy: 0.9480 - val_loss: 0.0481 - val_categorical_accuracy: 0.9940\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n",
      "padding maxlen= 250\n",
      "building model\n",
      "fitting model with max_features=10000, maxlen = 250, and dropout = 0.2\n",
      "Train on 114890 samples, validate on 12766 samples\n",
      "Epoch 1/4\n",
      "114890/114890 [==============================] - 1809s 16ms/step - loss: 0.0779 - categorical_accuracy: 0.9109 - val_loss: 0.0484 - val_categorical_accuracy: 0.9940\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04844, saving model to weights_base_10000_250_0.2.hdf5\n",
      "Epoch 2/4\n",
      "114890/114890 [==============================] - 1811s 16ms/step - loss: 0.0495 - categorical_accuracy: 0.9872 - val_loss: 0.0476 - val_categorical_accuracy: 0.9940\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.04844 to 0.04760, saving model to weights_base_10000_250_0.2.hdf5\n",
      "Epoch 3/4\n",
      "114890/114890 [==============================] - 1823s 16ms/step - loss: 0.0443 - categorical_accuracy: 0.9839 - val_loss: 0.0466 - val_categorical_accuracy: 0.9936\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.04760 to 0.04663, saving model to weights_base_10000_250_0.2.hdf5\n",
      "Epoch 4/4\n",
      "114890/114890 [==============================] - 1807s 16ms/step - loss: 0.0395 - categorical_accuracy: 0.9665 - val_loss: 0.0485 - val_categorical_accuracy: 0.9918\n",
      "\n",
      "Epoch 00004: val_loss did not improve\n"
     ]
    }
   ],
   "source": [
    "max_feature_list=[20000,10000]\n",
    "maxlen_list=[150,250] \n",
    "dropout_list=[0.1,0.2]\n",
    "EPOCHS=4\n",
    "\n",
    "for max_feature in [max_feature_list[1]]:\n",
    "    \n",
    "    for maxlen in maxlen_list:\n",
    "        \n",
    "        for dropout in dropout_list:\n",
    "            \n",
    "            if (maxlen,dropout) !=(150,0.1):  # already did 150,0.1\n",
    "                print('padding maxlen=',maxlen)\n",
    "                x_train_pad = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "                x_test_pad = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "\n",
    "                print('building model')\n",
    "                model=get_model(maxlen=maxlen,max_features=max_feature,dropout=dropout,embed_size=128)\n",
    "\n",
    "                batch_size = 32\n",
    "\n",
    "\n",
    "                weight_file_path=\"weights_base_{}_{}_{}.hdf5\".format(max_feature,maxlen,dropout)\n",
    "                model_file_path='bidirectional_lstm_globMP_relu_sigmoid_maxfeat{}_maxlen{}_dropout{}'.format(max_feature,maxlen,dropout)\n",
    "\n",
    "                checkpoint = ModelCheckpoint(weight_file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "                early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=20)\n",
    "\n",
    "                callbacks_list = [checkpoint, early] #early\n",
    "\n",
    "                #fit model\n",
    "                print('fitting model with max_features={}, maxlen = {}, and dropout = {}'.format(max_feature,maxlen,dropout))\n",
    "                model.fit(x_train_pad, y_train, batch_size=batch_size, epochs=EPOCHS, validation_split=0.1, callbacks=callbacks_list)\n",
    "\n",
    "                #save model\n",
    "                model.save(model_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 150 0.2\n",
      "10000 250 0.1\n",
      "10000 250 0.2\n"
     ]
    }
   ],
   "source": [
    "max_feature_list=[20000,10000]\n",
    "maxlen_list=[150,250] \n",
    "dropout_list=[0.1,0.2]\n",
    "EPOCHS=4\n",
    "\n",
    "for max_feature in [max_feature_list[1]]:\n",
    "    \n",
    "    for maxlen in maxlen_list:\n",
    "        \n",
    "        for dropout in dropout_list:\n",
    "            \n",
    "            if (maxlen,dropout) !=(150,0.1):\n",
    "                \n",
    "                print(max_feature,maxlen,dropout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict={'bidirectional_lstm_globMP_relu_sigmoid_maxfeat10000_maxlen150_dropout0.1':[150,0.1],\\\n",
    "'bidirectional_lstm_globMP_relu_sigmoid_maxfeat10000_maxlen150_dropout0.2':[150,0.2],\\\n",
    "'bidirectional_lstm_globMP_relu_sigmoid_maxfeat10000_maxlen250_dropout0.1':[250,0.1],\\\n",
    "'bidirectional_lstm_globMP_relu_sigmoid_maxfeat10000_maxlen250_dropout0.2':[250,0.2]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------- \n",
      "Model with max_features 10000, maxlen of 150, and dropout of 0.1\n",
      "padding maxlen= 150\n",
      "loading model\n",
      "compiling model\n",
      "evaluating model\n",
      "31915/31915 [==============================] - 84s 3ms/step\n",
      "categorical_accuracy: 99.27%\n",
      "-------------------------------------------------- \n",
      "\n",
      "-------------------------------------------------- \n",
      "Model with max_features 10000, maxlen of 150, and dropout of 0.2\n",
      "padding maxlen= 150\n",
      "loading model\n",
      "compiling model\n",
      "evaluating model\n",
      "31915/31915 [==============================] - 80s 3ms/step\n",
      "categorical_accuracy: 99.35%\n",
      "-------------------------------------------------- \n",
      "\n",
      "-------------------------------------------------- \n",
      "Model with max_features 10000, maxlen of 250, and dropout of 0.1\n",
      "padding maxlen= 250\n",
      "loading model\n",
      "compiling model\n",
      "evaluating model\n",
      "31915/31915 [==============================] - 126s 4ms/step\n",
      "categorical_accuracy: 99.35%\n",
      "-------------------------------------------------- \n",
      "\n",
      "-------------------------------------------------- \n",
      "Model with max_features 10000, maxlen of 250, and dropout of 0.2\n",
      "padding maxlen= 250\n",
      "loading model\n",
      "compiling model\n",
      "evaluating model\n",
      "31915/31915 [==============================] - 139s 4ms/step\n",
      "categorical_accuracy: 99.15%\n",
      "-------------------------------------------------- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model \n",
    "\n",
    "# Returns a compiled model identical to the previous one\n",
    "#model = load_model(\"model_name.hdf5\")\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=10000)\n",
    "#tokenize based on comment database\n",
    "tokenizer.fit_on_texts(list(train_all[\"comment_text\"].fillna(\"nada\").values)) #fit on all comment_text\n",
    "#create tokenized comments\n",
    "#list_tokenized_train = tokenizer.texts_to_sequences(x_train)  #training (80% of train.csv)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(x_test)  #testing (20% of train.csv)\n",
    "\n",
    "for file in model_dict.keys():\n",
    "    \n",
    "    maxlen=model_dict[file][0]\n",
    "    dropout=model_dict[file][1]\n",
    "    \n",
    "    print('-'*50,'\\nModel with max_features 10000, maxlen of {}, and dropout of {}'.format(maxlen,dropout))\n",
    "    \n",
    "    print('padding maxlen=',maxlen)\n",
    "    #x_train_pad = sequence.pad_sequences(list_tokenized_train, maxlen=model_dict[file])\n",
    "    x_test_pad = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "\n",
    "    print('loading model')\n",
    "    loaded_model=load_model(file)\n",
    "    \n",
    "    print('compiling model')\n",
    "    loaded_model.compile(loss='binary_crossentropy',optimizer='adam',metrics=[categorical_accuracy])\n",
    "    print('evaluating model')\n",
    "    score = loaded_model.evaluate(x_test_pad, y_test, verbose=1)\n",
    "    print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))\n",
    "    print('-'*50,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.topology.InputLayer at 0x16d47ebe0>,\n",
       " <keras.layers.embeddings.Embedding at 0x168198b38>,\n",
       " <keras.layers.wrappers.Bidirectional at 0x174c4cbe0>,\n",
       " <keras.layers.pooling.GlobalMaxPooling1D at 0x174c2bfd0>,\n",
       " <keras.layers.core.Dropout at 0x174c2bf98>,\n",
       " <keras.layers.core.Dense at 0x174c2bf60>,\n",
       " <keras.layers.core.Dropout at 0x16d08dd68>,\n",
       " <keras.layers.core.Dense at 0x174cc8da0>]"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model\n",
      "compiling model\n",
      "evaluating model\n",
      "31915/31915 [==============================] - 129s 4ms/step\n",
      "binary_accuracy: 98.31%\n",
      "-------------------------------------------------- \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('loading model')\n",
    "loaded_model=load_model('bidirectional_lstm_globMP_relu_sigmoid_maxfeat10000_maxlen250_dropout0.2')\n",
    "\n",
    "print('compiling model')\n",
    "loaded_model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['binary_accuracy','categorical_accuracy'])\n",
    "print('evaluating model')\n",
    "score = loaded_model.evaluate(x_test_pad, y_test, verbose=1)\n",
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[:], score[1]*100))\n",
    "print('-'*50,'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 4.92%\n",
      "binary_accuracy: 98.31%\n",
      "categorical_accuracy: 99.15%\n",
      "sparse_categorical_accuracy: 89.87%\n"
     ]
    }
   ],
   "source": [
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[0], score[0]*100))\n",
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))\n",
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[2], score[2]*100))\n",
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[3], score[3]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
