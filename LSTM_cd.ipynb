{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try some Keras LSTM bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from scipy.sparse import hstack\n",
    "from sklearn import model_selection\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ChrisD/anaconda3/envs/tensorflow/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow  as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "25000 train sequences\n",
      "25000 test sequences\n",
      "Pad sequences (samples x time)\n",
      "x_train shape: (25000, 100)\n",
      "x_test shape: (25000, 100)\n",
      "Train...\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/1\n",
      "25000/25000 [==============================] - 208s 8ms/step - loss: 0.4150 - acc: 0.8099 - val_loss: 0.3889 - val_acc: 0.8359\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x127badcf8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional\n",
    "from keras.datasets import imdb\n",
    "\n",
    "\n",
    "max_features = 20000\n",
    "# cut texts after this number of words\n",
    "# (among top max_features most common words)\n",
    "\n",
    "maxlen = 100\n",
    "batch_size = 32\n",
    "\n",
    "print('Loading data...')\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128, input_length=maxlen))\n",
    "model.add(Bidirectional(LSTM(64)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=1,\n",
    "          validation_data=[x_test, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('test_model.h5')  # creates a HDF5 file 'my_model.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loss', 'acc']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mean_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-c3a4fb1d84a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m model.compile(optimizer='rmsprop',\n\u001b[1;32m      2\u001b[0m               \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m               metrics=['accuracy', mean_pred])\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'mean_pred' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load our test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training data from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM_cd.ipynb              submission_logreg.csv\r\n",
      "README.md                  test.csv\r\n",
      "\u001b[34m__pycache__\u001b[m\u001b[m                toxic_comments.ipynb\r\n",
      "linear_regression_cd.ipynb train.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all=pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_all.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "toxic            15294\n",
       "severe_toxic      1595\n",
       "obscene           8449\n",
       "threat             478\n",
       "insult            7877\n",
       "identity_hate     1405\n",
       "dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_all[['toxic','severe_toxic','obscene','threat','insult','identity_hate']].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how few threats compared to toxic or obscene, severe_toxic ~= identity hate\n",
    "\n",
    "\n",
    "Below: count of other flags raised for all threat comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "toxic            449\n",
       "severe_toxic     112\n",
       "obscene          301\n",
       "threat           478\n",
       "insult           307\n",
       "identity_hate     98\n",
       "dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_all[train_all['threat']==1][['toxic','severe_toxic','obscene','threat','insult','identity_hate']].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "train_80, test_20 = model_selection.train_test_split(train_all,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_80, test_20 = model_selection.train_test_split(train_all,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "t80threats=train_80[train_80['threat']==1][['toxic','severe_toxic','obscene','threat','insult','identity_hate']].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "t20threats=test_20[test_20['threat']==1][['toxic','severe_toxic','obscene','threat','insult','identity_hate']].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "toxic            3.880435\n",
       "severe_toxic     4.333333\n",
       "obscene          3.426471\n",
       "threat           3.927835\n",
       "insult           3.385714\n",
       "identity_hate    3.454545\n",
       "dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t80threats/t20threats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So train and test subsamples have an approximately equal proportion of co-flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "toxic            3.935140\n",
       "severe_toxic     3.862805\n",
       "obscene          4.035161\n",
       "threat           3.927835\n",
       "insult           3.910848\n",
       "identity_hate    3.795222\n",
       "dtype: float64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_80[['toxic','severe_toxic','obscene','threat','insult','identity_hate']].sum()/ \\\n",
    "test_20[['toxic','severe_toxic','obscene','threat','insult','identity_hate']].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ratio of flags in general are between 3.80 and 4.04, so pretty even split of flags (though not necessarily an even split of comments with equivalent multi-category flags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max comment length in training set is 5000\n"
     ]
    }
   ],
   "source": [
    "maxlen=0.0\n",
    "for index, row in train_all.iterrows():\n",
    "    L=len(row['comment_text'])\n",
    "    if L > maxlen:\n",
    "        maxlen=L\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max comment char length in training set is 5000\n"
     ]
    }
   ],
   "source": [
    "print('max comment char length in training set is {}'.format(maxlen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxwords=0.0\n",
    "for index, row in train_all.iterrows():\n",
    "    L=len(row['comment_text'].split(' '))\n",
    "    if L > maxwords:\n",
    "        maxwords=L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max comment WORD COUNT (based on  split) in training set is 2273\n"
     ]
    }
   ],
   "source": [
    "print('max comment WORD COUNT (based on ' ' split) in training set is {}'.format(maxwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports:\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "127656 train sequences\n",
      "31915 test sequences\n"
     ]
    }
   ],
   "source": [
    "#preprocessing\n",
    "\n",
    "max_features = 5000\n",
    "# cut texts after this number of words, since character max is 5000, word max also has this ceiling\n",
    "\n",
    "print('Loading data...')\n",
    "\n",
    "#training data\n",
    "x_train = train_80['comment_text']\n",
    "y_train = train_80[['toxic','severe_toxic','obscene','threat','insult','identity_hate']]\n",
    "\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "#testing validation data (not for training model, just validation for )\n",
    "\n",
    "x_test = test_20['comment_text']\n",
    "y_test = test_20[['toxic','severe_toxic','obscene','threat','insult','identity_hate']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize comment_text\n",
    "Tokenizer= keras.preprocessing.text.Tokenizer(num_words=10000, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True, split=' ', char_level=False, oov_token=None)\n",
    "\n",
    "Tokenizer.fit_on_texts(train_all)\n",
    "x_train=Tokenizer.texts_to_sequences(x_train)\n",
    "x_test=Tokenizer.texts_to_sequences(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pad sequences (samples x time)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sequence' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-1371730a9287>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Pad sequences (samples x time)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mx_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mx_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'x_train shape:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sequence' is not defined"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "num_classes = 6\n",
    "batch_size = 50\n",
    "maxlen = 2300   # longest comment in words split(' ')\n",
    "\n",
    "\n",
    "\n",
    "print('Pad sequences (samples x time)')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "model = Sequential()  #Sequential model for LSTM\n",
    "\n",
    "#\n",
    "model.add(Embedding(max_features, 128, input_length=maxlen))\n",
    "model.add(Bidirectional(LSTM(64,activation='relu',recurrent_activation='hard_sigmoid')))  #relu for activation, hard sigmoid bc fast and reliable\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print('Train...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=4,\n",
    "          validation_data=[x_test, y_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package into single .py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting bidirectional_lstm_reluHiSig_softmax_toxic_test.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile bidirectional_lstm_reluHiSig_softmax_toxic_test.py\n",
    "#!~/anaconda3/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import pandas as pd\n",
    "import sys\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from scipy.sparse import hstack\n",
    "from sklearn import model_selection\n",
    "import numpy as np\n",
    "import tensorflow  as tf\n",
    "import keras\n",
    "import h5py\n",
    "from keras.models import load_model\n",
    "\n",
    "#imports:\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional\n",
    "\n",
    "#sys_argvs\n",
    "FILE=sys.argv[1]  # file path\n",
    "num_classes = 6\n",
    "batch_size = int(sys.argv[2]) # try with 32\n",
    "maxlen = int(sys.argv[3])   # longest comment in words split(' ') is 2300, vast majority under 300\n",
    "max_features = 5000 # cut texts after this number of words, since character max is 5000, word max also has this ceiling\n",
    "EPOCHS=int(sys.argv[4])\n",
    "\n",
    "# import data\n",
    "print('Importing data...')\n",
    "train_all=pd.read_csv(FILE)\n",
    "\n",
    "train_80, test_20 = model_selection.train_test_split(train_all,test_size=0.2)\n",
    "\n",
    "#preprocessing\n",
    "\n",
    "print('Splitting data...')\n",
    "\n",
    "#training data\n",
    "x_train = train_80['comment_text']\n",
    "y_train = train_80[['toxic','severe_toxic','obscene','threat','insult','identity_hate']]\n",
    "\n",
    "#testing validation data (not for training model, just validation for )\n",
    "\n",
    "x_test = test_20['comment_text']\n",
    "y_test = test_20[['toxic','severe_toxic','obscene','threat','insult','identity_hate']]\n",
    "\n",
    "# check lengths\n",
    "\n",
    "print(len(x_train), 'train sequences')\n",
    "print(len(x_test), 'test sequences')\n",
    "\n",
    "#Tokenize comment_text\n",
    "print('tokenizing')\n",
    "#top 10000 words, all lowercase\n",
    "Tokenizer= keras.preprocessing.text.Tokenizer(num_words=10000, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True, split=' ', char_level=False, oov_token=None)\n",
    "\n",
    "Tokenizer.fit_on_texts(train_all)\n",
    "x_train=Tokenizer.texts_to_sequences(x_train)\n",
    "x_test=Tokenizer.texts_to_sequences(x_test)\n",
    "\n",
    "\n",
    "print('Padding sequences')\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "#check shape\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 128, input_length=maxlen))\n",
    "model.add(Bidirectional(LSTM(64,activation='relu',recurrent_activation='hard_sigmoid')))  #relu for activation, hard sigmoid bc fast and reliable\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print('Training...')\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=EPOCHS,\n",
    "          validation_data=[x_test, y_test])\n",
    "\n",
    "model.save('bidirectional_lstm_reluHiSig_softmax_{}batch_{}epochs.h5'.format(batch_size,EPOCHS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ChrisD/anaconda3/envs/tensorflow/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "Importing data...\n",
      "Splitting data...\n",
      "127656 train sequences\n",
      "31915 test sequences\n",
      "tokenizing\n",
      "Padding sequences\n",
      "x_train shape: (127656, 300)\n",
      "x_test shape: (31915, 300)\n",
      "Training...\n",
      "Train on 127656 samples, validate on 31915 samples\n",
      "Epoch 1/2\n",
      "2018-04-15 13:16:14.278980: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      " 18850/127656 [===>..........................] - ETA: 29:46 - loss: 0.2343 - acc: 0.9614"
     ]
    }
   ],
   "source": [
    "!python bidirectional_lstm_reluHiSig_softmax_toxic_test.py train.csv 50 300 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting get_test_data.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile get_test_data.py\n",
    "#!~/anaconda3/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "!wget('https://s3.amazonaws.com/danicic-w266-final/train.csv')\n",
    "\n",
    "print('USE: !python bidirectional_lstm__reluHiSig_softmax_toxic_test.py train.csv <batch size> <maxlength> <epochs>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_lengths=list()\n",
    "\n",
    "for index, row in train_all.iterrows():\n",
    "    comment_lengths.append(len(row['comment_text'].split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.35546e+05, 1.59010e+04, 4.19700e+03, 1.45100e+03, 8.35000e+02,\n",
       "        6.97000e+02, 5.81000e+02, 2.85000e+02, 3.70000e+01, 1.40000e+01,\n",
       "        1.60000e+01, 3.00000e+00, 4.00000e+00, 1.00000e+00, 0.00000e+00,\n",
       "        0.00000e+00, 0.00000e+00, 2.00000e+00, 0.00000e+00, 1.00000e+00]),\n",
       " array([1.0000e+00, 1.1460e+02, 2.2820e+02, 3.4180e+02, 4.5540e+02,\n",
       "        5.6900e+02, 6.8260e+02, 7.9620e+02, 9.0980e+02, 1.0234e+03,\n",
       "        1.1370e+03, 1.2506e+03, 1.3642e+03, 1.4778e+03, 1.5914e+03,\n",
       "        1.7050e+03, 1.8186e+03, 1.9322e+03, 2.0458e+03, 2.1594e+03,\n",
       "        2.2730e+03]),\n",
       " <a list of 20 Patch objects>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(comment_lengths, bins=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9695120040608882"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([i for i in comment_lengths if i <301])/len(comment_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
